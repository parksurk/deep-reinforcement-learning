{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Genetic Algorithm\n",
    "### ref : Deep Reinforcement Learning _in Action_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "from torch.distributions import Bernoulli\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import halfnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a directory to store the MNIST dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a transformer to normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple linear classifier (or you can think of it as a single layer neural network). It simply multiplies a weight/parameter matrix by the input vector and applies a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(100,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Individual:\n",
    "    def __init__(self,param, fitness=0):\n",
    "        self.param = param\n",
    "        self.fitness = fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x,W):\n",
    "    return torch.nn.Softmax()(x @ W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.2397e-05, 4.3618e-02, 3.9825e-07, 3.7233e-05, 2.3996e-02, 3.0861e-01,\n",
       "         8.9450e-05, 6.2265e-01, 7.6238e-05, 8.6209e-04],\n",
       "        [2.4034e-05, 4.2048e-01, 2.3355e-03, 5.3658e-01, 1.9388e-04, 1.8655e-02,\n",
       "         4.9889e-05, 2.1546e-02, 1.2826e-04, 5.4047e-06],\n",
       "        [5.2195e-04, 1.8867e-01, 2.6529e-03, 3.6853e-03, 8.7694e-03, 7.5545e-01,\n",
       "         1.1094e-03, 3.4726e-02, 2.3029e-03, 2.1122e-03],\n",
       "        [1.7932e-03, 4.9951e-01, 1.8280e-02, 2.0288e-01, 8.4557e-02, 4.5482e-02,\n",
       "         6.6736e-04, 1.3456e-01, 1.1781e-02, 4.8024e-04],\n",
       "        [4.5312e-04, 7.3347e-01, 2.1193e-04, 3.8308e-03, 1.8762e-01, 3.8032e-03,\n",
       "         7.0501e-04, 6.8948e-02, 2.2618e-04, 7.3552e-04],\n",
       "        [3.2187e-02, 1.2166e-03, 5.9185e-07, 8.3157e-03, 3.4439e-04, 9.6708e-03,\n",
       "         4.8334e-04, 9.4752e-01, 5.0564e-05, 2.1575e-04],\n",
       "        [2.1903e-05, 1.0289e-03, 1.1728e-06, 4.1619e-05, 1.2273e-04, 6.3741e-04,\n",
       "         7.8936e-04, 9.9735e-01, 2.5888e-06, 3.1801e-06],\n",
       "        [8.0547e-04, 3.0429e-05, 4.3247e-07, 1.3625e-04, 7.7411e-06, 1.0211e-03,\n",
       "         9.3904e-06, 9.9798e-01, 5.9894e-07, 1.3375e-05],\n",
       "        [5.4325e-03, 4.3186e-03, 2.0992e-04, 1.2030e-03, 3.4787e-02, 4.7530e-02,\n",
       "         6.8152e-04, 9.0256e-01, 1.9657e-03, 1.3074e-03],\n",
       "        [1.2032e-05, 4.7243e-02, 8.4641e-06, 6.5845e-04, 3.5297e-03, 6.7570e-03,\n",
       "         9.1957e-06, 9.3950e-01, 1.0802e-05, 2.2723e-03],\n",
       "        [4.2961e-05, 1.7089e-03, 1.8601e-03, 2.1694e-03, 1.2238e-04, 1.8053e-04,\n",
       "         3.9958e-06, 9.9383e-01, 2.8565e-06, 7.4457e-05],\n",
       "        [2.6558e-04, 1.7993e-01, 5.0245e-05, 2.0393e-03, 1.8438e-01, 3.1418e-01,\n",
       "         8.1864e-04, 3.1254e-01, 4.5669e-04, 5.3384e-03],\n",
       "        [2.3110e-04, 1.4156e-02, 4.9538e-08, 3.5829e-05, 1.6953e-02, 8.5415e-01,\n",
       "         7.9811e-06, 1.1329e-01, 1.4535e-06, 1.1792e-03],\n",
       "        [8.4067e-04, 5.0163e-02, 1.3083e-04, 2.2079e-03, 6.5502e-04, 4.0481e-04,\n",
       "         8.8604e-04, 9.4468e-01, 2.5806e-05, 7.5915e-06],\n",
       "        [7.7078e-06, 5.3451e-01, 4.5551e-04, 1.3761e-02, 1.6922e-02, 2.4695e-01,\n",
       "         4.5875e-02, 1.3816e-01, 6.2161e-04, 2.7270e-03],\n",
       "        [6.2945e-03, 4.5079e-03, 1.1034e-05, 4.6067e-04, 7.9132e-04, 4.9969e-03,\n",
       "         2.9959e-04, 9.8099e-01, 3.4783e-04, 1.3020e-03],\n",
       "        [1.9763e-04, 2.5591e-01, 4.6911e-05, 1.9419e-04, 8.7632e-02, 5.5832e-01,\n",
       "         2.0857e-02, 7.2790e-02, 1.4432e-04, 3.9042e-03],\n",
       "        [7.0449e-05, 3.7545e-02, 2.2673e-05, 4.0697e-03, 1.1690e-02, 1.6365e-01,\n",
       "         3.6335e-04, 7.7706e-01, 1.2687e-03, 4.2573e-03],\n",
       "        [2.4233e-07, 4.5718e-02, 1.6515e-04, 4.3086e-04, 1.8578e-04, 1.4913e-02,\n",
       "         5.2347e-05, 9.3847e-01, 1.5481e-05, 5.0037e-05],\n",
       "        [3.9236e-06, 2.4433e-02, 3.2260e-04, 1.2530e-02, 4.4734e-04, 9.3928e-06,\n",
       "         6.8036e-05, 9.6218e-01, 8.6130e-07, 3.7785e-06],\n",
       "        [3.0540e-04, 1.3997e-02, 2.8638e-06, 1.8220e-03, 5.6417e-03, 1.3133e-01,\n",
       "         1.9791e-03, 8.4411e-01, 7.3059e-04, 7.7720e-05],\n",
       "        [1.5959e-05, 5.2176e-01, 3.8382e-05, 3.7677e-02, 3.3968e-03, 4.1950e-02,\n",
       "         3.3575e-04, 3.4497e-01, 5.4261e-04, 4.9317e-02],\n",
       "        [9.4347e-06, 1.6114e-01, 6.1822e-05, 3.8662e-04, 8.5563e-03, 2.4587e-03,\n",
       "         3.7720e-05, 8.2644e-01, 2.9265e-05, 8.8314e-04],\n",
       "        [2.0641e-05, 4.5275e-01, 6.7473e-04, 1.0936e-03, 1.4297e-01, 1.6570e-02,\n",
       "         1.3589e-04, 3.8144e-01, 4.1812e-03, 1.5611e-04],\n",
       "        [1.3322e-04, 8.5826e-01, 2.7755e-03, 2.4139e-04, 2.6946e-03, 1.1801e-01,\n",
       "         2.1150e-04, 4.8009e-03, 1.0600e-03, 1.1811e-02],\n",
       "        [1.6884e-04, 1.9004e-01, 4.5259e-04, 6.1396e-03, 1.3580e-03, 1.4937e-04,\n",
       "         6.4259e-04, 8.0100e-01, 4.8440e-06, 4.0274e-05],\n",
       "        [1.3382e-06, 8.0109e-02, 7.1929e-06, 4.8677e-05, 9.2940e-04, 8.7268e-01,\n",
       "         4.6062e-04, 4.5720e-02, 2.4345e-05, 1.6417e-05],\n",
       "        [5.9347e-05, 2.1569e-01, 1.6479e-04, 1.0886e-02, 6.2433e-02, 6.7657e-01,\n",
       "         9.1216e-03, 2.0203e-02, 3.4037e-03, 1.4765e-03],\n",
       "        [3.0226e-06, 2.9362e-01, 1.8867e-04, 2.6623e-04, 3.1065e-03, 2.1356e-01,\n",
       "         3.3601e-05, 4.8877e-01, 6.8134e-05, 3.7984e-04],\n",
       "        [1.9248e-06, 4.3321e-02, 5.4260e-06, 1.1695e-03, 7.7931e-05, 1.5384e-05,\n",
       "         4.4226e-05, 9.5536e-01, 2.5906e-07, 4.2688e-06],\n",
       "        [1.2738e-04, 4.9279e-01, 8.9684e-05, 1.4250e-04, 2.1559e-03, 8.7188e-02,\n",
       "         3.3374e-04, 1.3001e-02, 4.5187e-05, 4.0413e-01],\n",
       "        [1.2850e-04, 3.9716e-01, 1.8506e-03, 2.2356e-01, 7.5887e-04, 3.3418e-04,\n",
       "         2.1456e-04, 3.7585e-01, 3.5635e-05, 9.6057e-05],\n",
       "        [2.9854e-04, 1.4703e-01, 7.8426e-06, 5.5143e-01, 1.8113e-01, 1.0769e-02,\n",
       "         8.5190e-04, 1.0638e-01, 1.2345e-03, 8.6649e-04],\n",
       "        [6.8283e-06, 1.8334e-01, 4.5479e-05, 7.1106e-03, 2.8621e-02, 2.9193e-04,\n",
       "         5.0143e-04, 7.8002e-01, 2.8443e-05, 3.2425e-05],\n",
       "        [1.9679e-05, 6.6868e-01, 7.7295e-06, 8.9580e-05, 2.2682e-03, 2.2883e-01,\n",
       "         2.5735e-05, 9.8547e-02, 1.2478e-04, 1.3987e-03],\n",
       "        [1.2524e-03, 1.9438e-01, 7.1551e-04, 1.6064e-02, 2.8900e-03, 9.9396e-03,\n",
       "         1.8780e-03, 7.3921e-01, 3.0228e-04, 3.3371e-02],\n",
       "        [4.0023e-02, 2.4485e-01, 3.7188e-04, 4.6025e-02, 8.7662e-03, 2.7569e-02,\n",
       "         7.0420e-03, 6.2202e-01, 7.5278e-05, 3.2572e-03],\n",
       "        [1.7392e-03, 3.2425e-02, 9.1166e-04, 6.3844e-02, 3.8627e-01, 3.2136e-01,\n",
       "         1.0126e-03, 1.6085e-01, 3.0818e-02, 7.7627e-04],\n",
       "        [1.4938e-05, 3.8868e-03, 1.1984e-07, 1.0718e-05, 5.0535e-03, 7.8962e-01,\n",
       "         5.4856e-05, 2.0096e-01, 1.2513e-05, 3.8767e-04],\n",
       "        [2.9630e-05, 1.1958e-02, 1.7142e-08, 1.2862e-03, 9.6342e-03, 8.3502e-02,\n",
       "         1.3026e-04, 8.9271e-01, 2.2295e-05, 7.2294e-04],\n",
       "        [1.4705e-07, 1.7451e-02, 6.0258e-06, 3.8790e-05, 1.5684e-03, 9.0371e-01,\n",
       "         7.8686e-05, 7.7108e-02, 1.7433e-05, 2.1484e-05],\n",
       "        [6.4534e-06, 8.9945e-01, 3.3117e-06, 2.9992e-02, 1.2667e-04, 2.8016e-03,\n",
       "         2.3895e-04, 6.6677e-02, 4.0220e-05, 6.5935e-04],\n",
       "        [1.0018e-05, 1.3217e-03, 2.2578e-08, 9.0202e-06, 1.6650e-03, 2.8019e-01,\n",
       "         5.7226e-06, 7.1675e-01, 2.5560e-06, 4.4303e-05],\n",
       "        [6.6963e-03, 1.8205e-01, 3.4257e-06, 1.4095e-03, 3.5112e-02, 7.3072e-01,\n",
       "         5.2035e-04, 4.1195e-02, 9.4768e-04, 1.3442e-03],\n",
       "        [8.3651e-04, 7.5472e-02, 3.4794e-05, 2.6667e-04, 3.8635e-02, 3.4937e-02,\n",
       "         3.6171e-04, 7.6444e-01, 1.1194e-04, 8.4903e-02],\n",
       "        [1.4299e-04, 6.7723e-01, 1.0578e-05, 4.1217e-03, 7.6995e-03, 2.3192e-01,\n",
       "         1.3958e-04, 5.1976e-02, 1.5535e-04, 2.6599e-02],\n",
       "        [2.1749e-05, 6.8532e-03, 3.0882e-06, 3.2026e-04, 1.2413e-02, 9.1566e-02,\n",
       "         1.6635e-04, 8.8771e-01, 4.0922e-04, 5.3332e-04],\n",
       "        [2.4064e-04, 5.5604e-02, 2.5513e-04, 3.2980e-04, 6.8470e-03, 1.2702e-02,\n",
       "         6.1938e-04, 9.2162e-01, 1.0942e-04, 1.6772e-03],\n",
       "        [1.5695e-05, 3.4482e-01, 4.4857e-03, 6.0171e-04, 5.9453e-02, 2.2859e-01,\n",
       "         2.4460e-05, 3.6177e-01, 1.9881e-04, 4.1616e-05],\n",
       "        [6.9640e-04, 6.5780e-01, 3.2389e-03, 7.2249e-02, 2.2005e-02, 5.8014e-04,\n",
       "         8.9939e-04, 2.4146e-01, 7.1554e-05, 1.0009e-03],\n",
       "        [3.0459e-03, 6.8932e-01, 1.9424e-03, 2.2025e-02, 7.1756e-03, 1.0270e-03,\n",
       "         1.2200e-03, 2.7389e-01, 2.2303e-04, 1.2950e-04],\n",
       "        [8.9223e-07, 1.2455e-02, 1.5478e-06, 3.3469e-05, 7.2324e-03, 9.2800e-01,\n",
       "         4.9396e-05, 5.2112e-02, 3.7282e-05, 7.3793e-05],\n",
       "        [6.6656e-02, 1.0158e-01, 5.4946e-05, 9.6835e-02, 3.7122e-02, 2.2737e-02,\n",
       "         2.5588e-04, 6.7130e-01, 3.4679e-05, 3.4316e-03],\n",
       "        [2.0017e-03, 4.6161e-01, 9.0004e-05, 3.8939e-04, 4.8741e-02, 1.1338e-03,\n",
       "         9.2895e-03, 4.7537e-01, 1.1782e-04, 1.2563e-03],\n",
       "        [4.6620e-04, 2.1262e-01, 2.4936e-04, 4.9005e-02, 1.5849e-03, 2.5414e-04,\n",
       "         1.1090e-04, 7.3544e-01, 1.2639e-04, 1.4952e-04],\n",
       "        [1.3833e-07, 6.7802e-02, 1.0280e-04, 5.2222e-04, 1.6398e-04, 1.0964e-03,\n",
       "         2.1077e-04, 9.3003e-01, 3.9049e-05, 2.8880e-05],\n",
       "        [1.8190e-05, 1.2239e-01, 1.8923e-05, 4.6456e-04, 3.7692e-03, 8.2777e-01,\n",
       "         2.4148e-04, 2.8055e-02, 5.9094e-04, 1.6686e-02],\n",
       "        [7.9713e-04, 1.9307e-01, 2.5405e-03, 1.5314e-02, 2.1273e-01, 4.7104e-02,\n",
       "         3.7937e-03, 4.9008e-01, 2.9601e-02, 4.9746e-03],\n",
       "        [7.0585e-06, 7.5404e-01, 2.8071e-06, 1.9016e-03, 8.4739e-04, 5.5700e-02,\n",
       "         3.8172e-05, 1.8737e-01, 3.6139e-05, 6.1022e-05],\n",
       "        [3.8802e-04, 6.2466e-01, 5.8392e-03, 1.8131e-02, 4.1828e-02, 2.4252e-04,\n",
       "         2.8231e-04, 3.0538e-01, 3.3366e-04, 2.9084e-03],\n",
       "        [3.2761e-04, 1.4462e-01, 2.1080e-03, 1.3838e-02, 8.3846e-03, 4.3708e-01,\n",
       "         1.2513e-04, 3.7649e-01, 2.0484e-04, 1.6827e-02],\n",
       "        [8.0869e-05, 9.3580e-02, 5.4171e-07, 4.1345e-05, 2.1220e-02, 1.8018e-01,\n",
       "         1.9537e-04, 7.0394e-01, 7.9097e-05, 6.8467e-04],\n",
       "        [9.9409e-07, 4.8105e-02, 4.9423e-05, 5.9167e-05, 1.0643e-03, 9.1421e-01,\n",
       "         2.8581e-05, 3.6406e-02, 5.6049e-05, 1.8443e-05],\n",
       "        [2.7987e-05, 8.5059e-01, 3.9447e-06, 1.1100e-03, 1.5952e-02, 8.3514e-02,\n",
       "         5.0945e-06, 4.8445e-02, 7.4943e-05, 2.7518e-04],\n",
       "        [1.7342e-05, 1.0902e-01, 1.9639e-04, 4.2505e-04, 9.8145e-03, 4.9124e-05,\n",
       "         5.5247e-04, 8.7990e-01, 1.7696e-06, 2.2600e-05],\n",
       "        [2.7320e-06, 3.8322e-03, 1.1151e-06, 1.7327e-05, 9.0645e-03, 2.0455e-03,\n",
       "         1.8176e-06, 9.8499e-01, 3.5316e-07, 4.3516e-05],\n",
       "        [6.8048e-05, 9.8121e-01, 5.7432e-05, 9.9643e-04, 8.0868e-03, 3.4569e-03,\n",
       "         1.0078e-03, 1.6734e-03, 1.6312e-03, 1.8140e-03],\n",
       "        [3.9598e-05, 1.5074e-01, 2.1917e-05, 3.0563e-03, 4.9922e-02, 1.0837e-01,\n",
       "         3.3277e-04, 6.8214e-01, 1.5478e-03, 3.8264e-03],\n",
       "        [3.4693e-06, 7.4856e-01, 7.8427e-07, 6.5206e-04, 3.0186e-03, 1.2532e-01,\n",
       "         4.3762e-05, 1.2204e-01, 3.2373e-05, 3.2040e-04],\n",
       "        [1.6143e-05, 5.0185e-05, 1.9113e-07, 9.1536e-07, 4.6800e-05, 5.9884e-03,\n",
       "         1.6583e-05, 9.9384e-01, 2.4401e-07, 4.4612e-05],\n",
       "        [1.7199e-04, 2.2589e-01, 6.7506e-05, 2.3466e-03, 6.4507e-02, 6.2964e-02,\n",
       "         5.9678e-04, 6.3761e-01, 1.5346e-03, 4.3163e-03],\n",
       "        [5.4803e-04, 2.8485e-01, 7.5133e-03, 2.7366e-02, 6.9931e-02, 5.1816e-01,\n",
       "         3.4437e-03, 7.5791e-02, 6.7959e-03, 5.6034e-03],\n",
       "        [4.0597e-05, 7.1051e-01, 7.9939e-05, 2.5484e-03, 4.9775e-02, 1.2150e-01,\n",
       "         6.5770e-05, 1.1421e-01, 6.5029e-04, 6.2117e-04],\n",
       "        [5.8568e-06, 7.0950e-05, 1.0078e-04, 7.1953e-05, 6.1460e-06, 7.5501e-04,\n",
       "         2.2635e-05, 9.9896e-01, 8.9705e-08, 8.6649e-06],\n",
       "        [2.8970e-05, 6.3171e-01, 6.4557e-03, 3.0430e-03, 7.2965e-03, 2.6527e-01,\n",
       "         4.4330e-05, 8.4828e-02, 9.0776e-04, 4.2495e-04],\n",
       "        [5.2425e-05, 8.6082e-02, 5.1639e-04, 2.9314e-03, 2.4686e-02, 6.0702e-02,\n",
       "         7.3111e-04, 8.1065e-01, 5.9703e-04, 1.3047e-02],\n",
       "        [9.6392e-06, 1.3132e-01, 1.7290e-04, 1.1421e-02, 5.2021e-02, 3.1049e-02,\n",
       "         1.3490e-04, 7.3966e-01, 4.2394e-04, 3.3783e-02],\n",
       "        [9.4639e-04, 2.7488e-01, 1.7758e-04, 7.6059e-03, 7.6846e-04, 3.0016e-03,\n",
       "         7.5837e-03, 6.9824e-01, 4.8112e-06, 6.7878e-03],\n",
       "        [6.1061e-04, 4.3257e-01, 6.3170e-05, 5.6470e-03, 3.4576e-02, 3.3262e-01,\n",
       "         2.6928e-03, 1.8988e-01, 7.3814e-04, 6.0679e-04],\n",
       "        [7.6064e-05, 9.7632e-01, 1.0806e-05, 1.1709e-04, 1.7400e-04, 2.6149e-03,\n",
       "         5.4929e-04, 1.7672e-02, 1.6481e-05, 2.4540e-03],\n",
       "        [5.2007e-04, 2.4133e-02, 2.2990e-04, 8.0171e-04, 2.0506e-03, 3.0764e-01,\n",
       "         4.8432e-04, 6.6310e-01, 5.3582e-06, 1.0321e-03],\n",
       "        [2.3954e-05, 5.5411e-01, 5.6109e-04, 1.5521e-02, 4.8809e-02, 2.9328e-01,\n",
       "         3.8846e-04, 8.0415e-02, 3.6568e-04, 6.5246e-03],\n",
       "        [8.3441e-05, 2.7556e-02, 6.3626e-04, 1.9875e-04, 2.3941e-04, 1.8201e-02,\n",
       "         3.2413e-04, 9.5248e-01, 2.7551e-04, 7.3540e-06],\n",
       "        [1.2688e-06, 9.7669e-01, 9.4024e-05, 9.5184e-03, 6.8504e-04, 8.4292e-04,\n",
       "         6.0021e-04, 1.0501e-02, 6.7999e-05, 9.9644e-04],\n",
       "        [1.0716e-05, 2.4311e-01, 6.4767e-05, 1.5521e-03, 1.4865e-01, 5.7301e-03,\n",
       "         8.2311e-04, 5.9356e-01, 1.0839e-04, 6.3904e-03],\n",
       "        [7.3194e-05, 5.9318e-03, 1.2480e-04, 1.1972e-04, 2.0261e-03, 2.6547e-03,\n",
       "         2.2744e-06, 9.8906e-01, 5.4212e-06, 5.8559e-06],\n",
       "        [6.2270e-05, 8.5059e-01, 2.5927e-05, 1.1805e-02, 1.7125e-03, 9.0749e-02,\n",
       "         1.3468e-03, 4.0641e-02, 3.9311e-04, 2.6720e-03],\n",
       "        [6.8926e-05, 4.8122e-03, 2.9806e-07, 4.0517e-05, 1.2930e-03, 4.1627e-01,\n",
       "         2.3144e-05, 5.7716e-01, 1.1154e-05, 3.2759e-04],\n",
       "        [1.6961e-03, 1.0857e-01, 2.2330e-05, 3.3744e-03, 9.5568e-03, 9.7764e-04,\n",
       "         1.6347e-02, 8.4944e-01, 9.1441e-03, 8.7686e-04],\n",
       "        [4.7180e-06, 9.7362e-01, 1.8903e-05, 1.9306e-04, 1.1172e-03, 9.5730e-03,\n",
       "         8.7247e-05, 1.1538e-02, 5.6219e-06, 3.8421e-03],\n",
       "        [4.2167e-06, 9.8525e-01, 1.0258e-05, 2.4623e-04, 1.9900e-04, 7.4732e-03,\n",
       "         9.5711e-05, 3.5456e-03, 2.5936e-04, 2.9171e-03],\n",
       "        [2.1192e-04, 1.5112e-06, 2.2747e-07, 5.4375e-07, 1.1881e-05, 1.4222e-04,\n",
       "         1.2570e-07, 9.9963e-01, 6.8097e-08, 5.7887e-06],\n",
       "        [3.7972e-07, 2.0642e-02, 1.8505e-05, 9.5424e-05, 9.3096e-03, 9.3306e-01,\n",
       "         2.4535e-06, 3.6843e-02, 4.5194e-06, 2.6617e-05],\n",
       "        [1.1641e-03, 8.5745e-01, 5.7076e-04, 5.5248e-03, 4.5698e-03, 3.9531e-03,\n",
       "         2.3659e-03, 1.2237e-01, 5.6352e-05, 1.9806e-03],\n",
       "        [2.2263e-05, 9.0993e-04, 3.0052e-06, 1.4627e-04, 6.7348e-04, 1.1599e-03,\n",
       "         1.3414e-04, 9.9691e-01, 3.9445e-05, 2.5112e-06],\n",
       "        [1.5459e-03, 2.0746e-02, 5.2926e-04, 9.4216e-03, 3.6540e-02, 7.7969e-02,\n",
       "         1.6262e-02, 8.1252e-01, 1.4066e-02, 1.0397e-02],\n",
       "        [2.6460e-04, 5.4571e-01, 9.6888e-05, 2.0266e-02, 4.6481e-02, 8.0642e-02,\n",
       "         1.3270e-01, 1.7215e-01, 3.8152e-04, 1.3092e-03],\n",
       "        [1.1767e-04, 6.4018e-02, 2.3955e-04, 2.9096e-05, 7.6557e-04, 1.2736e-01,\n",
       "         3.8325e-03, 7.8981e-01, 2.3066e-06, 1.3823e-02],\n",
       "        [1.2479e-04, 4.8824e-01, 4.9523e-04, 1.6644e-03, 2.0466e-03, 2.0031e-01,\n",
       "         7.4865e-04, 2.9107e-01, 9.6225e-04, 1.4332e-02],\n",
       "        [2.9304e-05, 2.1497e-01, 2.2120e-04, 4.2666e-03, 6.2731e-02, 1.9397e-02,\n",
       "         1.4805e-02, 6.8342e-01, 2.1681e-05, 1.4018e-04]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x,torch.rand(784,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spawn_population(param_size=(784,10),pop_size=1000):\n",
    "    return [Individual(torch.randn(*param_size)) for i in range(pop_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_population(pop):\n",
    "    avg_fit = 0 #avg population fitness\n",
    "    for individual in pop:\n",
    "        x,y = next(iter(train_loader))\n",
    "        pred = model(x.reshape(batch_size,784),individual.param)\n",
    "        loss = loss_fn(pred,y)\n",
    "        fit = loss\n",
    "        individual.fitness = 1.0 / fit\n",
    "        avg_fit += fit\n",
    "    avg_fit = avg_fit / len(pop)\n",
    "    return pop, avg_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop[0].param.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.stack((pop[0].param.view(-1),pop[1].param.view(-1)),dim=0).view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recombine(x1,x2): #x1,x2 : Individual\n",
    "    w1 = x1.param.view(-1) #flatten\n",
    "    w2 = x2.param.view(-1)\n",
    "    cross_pt = random.randint(0,w1.shape[0])\n",
    "    child1 = torch.zeros(w1.shape)\n",
    "    child2 = torch.zeros(w1.shape)\n",
    "    child1[0:cross_pt] = w1[0:cross_pt]\n",
    "    child1[cross_pt:] = w2[cross_pt:]\n",
    "    child2[0:cross_pt] = w2[0:cross_pt]\n",
    "    child2[cross_pt:] = w1[cross_pt:]\n",
    "    child1 = child1.reshape(784,10)\n",
    "    child2 = child2.reshape(784,10)\n",
    "    c1 = Individual(child1)\n",
    "    c2 = Individual(child2)\n",
    "    return [c1,c2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(pop, mut_rate=0.01):\n",
    "    param_shape = pop[0].param.shape\n",
    "    l = torch.zeros(*param_shape)\n",
    "    l[:] = mut_rate\n",
    "    m = Bernoulli(l)\n",
    "    for individual in pop:\n",
    "        mut_vector = m.sample() * torch.randn(*param_shape)\n",
    "        individual.param = mut_vector + individual.param\n",
    "    return pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_next_population(pop,pop_size=1000, mut_rate=0.01):\n",
    "    new_pop = []\n",
    "    while len(new_pop) < pop_size: #until new pop is full\n",
    "        parents = random.choices(pop,k=2, weights=[x.fitness for x in pop])\n",
    "        offspring = recombine(parents[0],parents[1])\n",
    "        new_pop.extend(offspring)\n",
    "    new_pop = mutate(new_pop,mut_rate)\n",
    "    return new_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = spawn_population()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.2 s, sys: 642 ms, total: 54.9 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pop, avg_fit = evaluate_population(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pop = seed_next_population(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to spawn a population of weight matrices, run the model using the different individuals, calculate the loss for each one, and then breed the ones with the highest fitness score (lowest loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_generations = 50\n",
    "population_size = 100\n",
    "mutation_rate = 0.001 # 1% mutation rate per generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Evolution (Training) Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_fit = []\n",
    "pop = spawn_population(pop_size=population_size) #initial population\n",
    "for gen in range(num_generations):\n",
    "    # trainning\n",
    "    pop, avg_fit = evaluate_population(pop)\n",
    "    pop_fit.append(avg_fit) #record population average fitness\n",
    "    new_pop = seed_next_population(pop, pop_size=population_size, mut_rate=mutation_rate)\n",
    "    pop = new_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pop_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = 0\n",
    "for i in range(len(pop)):\n",
    "    x,y = next(iter(train_loader))\n",
    "    pred = model(x.reshape(batch_size,784),pop[i].param)\n",
    "    loss = loss_fn(pred,y)\n",
    "    avg_loss += loss\n",
    "avg_loss /= len(pop)\n",
    "print(avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avg Loss new pop: 2.3336\n",
    "Avg Loss after 10 gens: 2.3435"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with gradient-descent (comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.randn(784,10, requires_grad=True)\n",
    "optim = torch.optim.Adam(lr=0.1, params=[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "for i in range(50):\n",
    "    for x,y in train_loader:\n",
    "        optim.zero_grad()\n",
    "        pred = model(x.reshape(batch_size,784),p)\n",
    "        loss = loss_fn(pred,y)\n",
    "        loss_list.append(loss)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(loss)\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
