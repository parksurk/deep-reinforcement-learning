
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{3. Policy-Based Methods}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
        \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}0\PYZhy{}0\PYZhy{}0\PYZus{}opening.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}1}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_0_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{learning-plan}{%
\section{Learning Plan}\label{learning-plan}}

    \hypertarget{lesson-3-1-introduction-to-policy-based-methods}{%
\subsubsection{Lesson 3-1: Introduction to Policy-Based
Methods}\label{lesson-3-1-introduction-to-policy-based-methods}}

In this lesson, you will learn about methods such as hill climbing,
simulated annealing, and adaptive noise scaling. You'll also learn about
cross-entropy methods and evolution strategies.

\hypertarget{lesson-3-2-policy-gradient-methods}{%
\subsubsection{Lesson 3-2: : Policy Gradient
Methods}\label{lesson-3-2-policy-gradient-methods}}

In this lesson, you'll study REINFORCE, along with improvements we can
make to lower the variance of policy gradient algorithms.

\hypertarget{lesson-3-3-proximal-policy-optimization}{%
\subsubsection{Lesson 3-3: : Proximal Policy
Optimization}\label{lesson-3-3-proximal-policy-optimization}}

In this lesson, you'll learn about Proximal Policy Optimization (PPO), a
cutting-edge policy gradient method.

\hypertarget{lesson-3-4-actor-critic-methods}{%
\subsubsection{Lesson 3-4: : Actor-Critic
Methods}\label{lesson-3-4-actor-critic-methods}}

In this lesson, you'll learn how to combine value-based and policy-based
methods, bringing together the best of both worlds, to solve challenging
reinforcement learning problems.

\hypertarget{lesson-3-5-deep-rl-for-finance-optional}{%
\subsubsection{Lesson 3-5: : Deep RL for Finance
(Optional)}\label{lesson-3-5-deep-rl-for-finance-optional}}

In this optional lesson, you'll learn how to apply deep reinforcement
learning techniques for optimal execution of portfolio transactions.

    \hypertarget{optional-resorces}{%
\subsection{Optional Resorces}\label{optional-resorces}}

\begin{itemize}
\tightlist
\item
  Read the most famous {[}blog post{]}
  \{http://karpathy.github.io/2016/05/31/rl/\} on policy gradient
  methods.
\item
  Implement a policy gradient method to win at Pong in this {[}Medium
  post{]}
  \{https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0\}.
\item
  Learn more about {[}evolution strategies{]}
  \{https://blog.openai.com/evolution-strategies/\} from OpenAI.
\end{itemize}

    \hypertarget{lesson-3-1-introduction-to-policy-based-methods}{%
\section{Lesson 3-1: Introduction to Policy-Based
Methods}\label{lesson-3-1-introduction-to-policy-based-methods}}

    \hypertarget{policy-based-methods}{%
\subsection{3-1-1 : Policy-Based Methods}\label{policy-based-methods}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
        \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}1\PYZhy{}1\PYZus{}value\PYZus{}based\PYZus{}methods\PYZus{}with\PYZus{}discrete\PYZus{}state.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}2}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_6_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
        \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}1\PYZhy{}2\PYZus{}value\PYZus{}based\PYZus{}methods\PYZus{}with\PYZus{}continuous\PYZus{}state.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}3}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_7_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{its-impossible-to-represnet-the-optimal-action-value-fruction-in-a-table.}{%
\subsubsection{It's impossible to represnet the optimal action value
fruction in a
table.}\label{its-impossible-to-represnet-the-optimal-action-value-fruction-in-a-table.}}

\hypertarget{because-we-need-to-a-row-from-the-table-and-that-would-make-the-table-too-big-to-be-useful-in-practice.}{%
\subsubsection{Because we need to a row from the table and that would
make the table too big to be useful in
practice.}\label{because-we-need-to-a-row-from-the-table-and-that-would-make-the-table-too-big-to-be-useful-in-practice.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
        \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}1\PYZhy{}3\PYZus{}value\PYZus{}based\PYZus{}methods\PYZus{}with\PYZus{}continuous\PYZus{}state\PYZus{}using\PYZus{}Deep\PYZus{}Q\PYZus{}Learning.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}4}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_9_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
        \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}1\PYZhy{}4\PYZus{}value\PYZus{}based\PYZus{}methods\PYZus{}estimate\PYZus{}optimal\PYZus{}value\PYZus{}function\PYZus{}first\PYZus{}before\PYZus{}optimal\PYZus{}policy.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}5}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_10_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{important-message-is-that}{%
\subsubsection{Important message is
that\ldots{}}\label{important-message-is-that}}

\hypertarget{in-both-cases-whether-we-used-a-table-for-small-state-spaces-or-a-neural-network-for-much-larger-state-spaces}{%
\subsubsection{in both cases, whether we used a table for small state
spaces or a neural network for much larger state
spaces,}\label{in-both-cases-whether-we-used-a-table-for-small-state-spaces-or-a-neural-network-for-much-larger-state-spaces}}

\hypertarget{we-had-to-first-estimate-the-optimal-action-value-fuction}{%
\subsection{We had to first estimate the `Optimal Action Value
Fuction'}\label{we-had-to-first-estimate-the-optimal-action-value-fuction}}

\hypertarget{before-we-could-make-the-optimal-policy}{%
\subsection{before we could make the `Optimal
Policy'}\label{before-we-could-make-the-optimal-policy}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
        \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}1\PYZhy{}5\PYZus{}policy\PYZus{}based\PYZus{}methods.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}6}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_12_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{key-concept-about-policy-based-methods-is}{%
\subsubsection{Key concept about `Policy-based Methods'
is\ldots{}}\label{key-concept-about-policy-based-methods-is}}

\hypertarget{we-can-directly-find-the-optimal-policy-without-worrying-about-a-value-fuction-at-all.}{%
\subsection{We can directly find the `Optimal Policy' without worrying
about a value fuction at
all.}\label{we-can-directly-find-the-optimal-policy-without-worrying-about-a-value-fuction-at-all.}}

    \hypertarget{policy-function-approximation}{%
\subsection{3-1-2 : Policy Function
Approximation}\label{policy-function-approximation}}

\hypertarget{how-might-we-use-a-neural-network-to-approximate-a-policy}{%
\subsubsection{How might we use a neural network to approximate a
policy?}\label{how-might-we-use-a-neural-network-to-approximate-a-policy}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
        \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}2\PYZhy{}1\PYZus{}policy\PYZus{}function\PYZus{}approximation\PYZus{}cartpole\PYZus{}has\PYZus{}two\PYZus{}actions.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}7}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
        \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}2\PYZhy{}2\PYZus{}policy\PYZus{}function\PYZus{}approximation\PYZus{}neural\PYZus{}newtwork\PYZus{}return\PYZus{}posibilities.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}8}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_16_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{our-objective-is-to-determine-appropriate-values-for-the-network-weights-so-that-for-each-state-that-we-pass-into-the-network.}{%
\subsubsection{Our objective is to determine appropriate values for the
network weights so that for each state that we pass into the
network.}\label{our-objective-is-to-determine-appropriate-values-for-the-network-weights-so-that-for-each-state-that-we-pass-into-the-network.}}

\hypertarget{neural-network-returns-action-probabilities-where-the-optimal-action-is-most-likely-to-be-selected.}{%
\subsection{Neural Network returns action probabilities where the
optimal action is most likely to be
selected.}\label{neural-network-returns-action-probabilities-where-the-optimal-action-is-most-likely-to-be-selected.}}

\hypertarget{this-will-help-the-agent-with-its-goal-to-maximize-expected-return.}{%
\subsubsection{This will help the agent with its goal to maximize
expected
return.}\label{this-will-help-the-agent-with-its-goal-to-maximize-expected-return.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
        \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}2\PYZhy{}3\PYZus{}policy\PYZus{}function\PYZus{}approximation\PYZus{}agent\PYZus{}learns\PYZus{}how\PYZus{}to\PYZus{}maximize\PYZus{}reword\PYZus{}interactively.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}9}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_18_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{weights-are-initially-set-to-random-values.}{%
\subsubsection{1. Weights are initially set to random
values.}\label{weights-are-initially-set-to-random-values.}}

\hypertarget{the-agent-interacts-with-the-environment}{%
\subsubsection{2. The agent interacts with the
environment}\label{the-agent-interacts-with-the-environment}}

\hypertarget{the-agnet-learns-more-about-what-strategies-are-best-for-maximizing-reward.}{%
\subsubsection{3. The agnet learns more about what strategies are best
for maximizing
reward.}\label{the-agnet-learns-more-about-what-strategies-are-best-for-maximizing-reward.}}

\hypertarget{this-process-amends-those-wheights-and-the-agent-starts-to-choose-the-appropriate-action-for-each-state-and-gradually-masters-the-cartpole-task.}{%
\subsection{This process amends those wheights and the agent starts to
choose the appropriate action for each state and gradually masters the
Cartpole
task.}\label{this-process-amends-those-wheights-and-the-agent-starts-to-choose-the-appropriate-action-for-each-state-and-gradually-masters-the-cartpole-task.}}

    \hypertarget{more-on-the-policy}{%
\subsection{3-1-3 : More on the Policy}\label{more-on-the-policy}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}3\PYZhy{}1\PYZus{}neural\PYZus{}network\PYZus{}encodes\PYZus{}action\PYZus{}probabilities.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}10}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_21_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    {[}ref{]} \{https://blog.openai.com/evolution-strategies/\}

\hypertarget{neural-network-approximates-stochastic-policy}{%
\subsubsection{Neural Network approximates {[}Stochastic
Policy{]}}\label{neural-network-approximates-stochastic-policy}}

\begin{itemize}
\tightlist
\item
  Above image, a simple neural network architecture to approximate a
  stochastic policy.

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    The agent passes the current environment state as input to the
    network.
  \item
    The neural network returns `action probabilities'.
  \item
    The agent samples from those probabilities to select an action.
  \end{enumerate}
\end{itemize}

\hypertarget{neural-network-approximates-deterministic-policy}{%
\subsubsection{Neural Network approximates {[}Deterministic
Policy{]}}\label{neural-network-approximates-deterministic-policy}}

\begin{itemize}
\tightlist
\item
  Instead of sampling from the action probabilites, the agent need only
  choose the greedy action.
\end{itemize}

    \hypertarget{quiz}{%
\subsection{Quiz}\label{quiz}}

\hypertarget{question-you-learned-that-the-neural-network-that-approximates-the-policy-takes-the-environment-state-as-input.-the-output-layer-returns-the-probability-that-the-agent-should-select-each-possible-action.-which-of-the-following-is-a-valid-activation-function-for-the-output-layer}{%
\subsubsection{Question : you learned that the neural network that
approximates the policy takes the environment state as input. The output
layer returns the probability that the agent should select each possible
action. Which of the following is a valid activation function for the
output
layer?}\label{question-you-learned-that-the-neural-network-that-approximates-the-policy-takes-the-environment-state-as-input.-the-output-layer-returns-the-probability-that-the-agent-should-select-each-possible-action.-which-of-the-following-is-a-valid-activation-function-for-the-output-layer}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  linear (i.e., no activation function)
\item
  softmax
\item
  ReLu
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}3\PYZhy{}2\PYZus{}continuous\PYZus{}action\PYZus{}space\PYZus{}in\PYZus{}bipedal\PYZus{}walker.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}11}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_24_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{question-is}{%
\subsubsection{Question is\ldots{}}\label{question-is}}

\hypertarget{cartpole-environment-has-a-discrete-action-space.-so-how-do-we-use-a-neural-network-to-approximate-a-policy-if-the-environment-has-a-continuous-action-space}{%
\subsubsection{CartPole environment has a discrete action space. So, how
do we use a neural network to approximate a policy, if the environment
has a continuous action
space?}\label{cartpole-environment-has-a-discrete-action-space.-so-how-do-we-use-a-neural-network-to-approximate-a-policy-if-the-environment-has-a-continuous-action-space}}

\hypertarget{discrete-action-spaces-the-neural-network-has-one-node-for-each-possible-action.}{%
\subsection{* Discrete Action spaces : the neural network has one node
for each possible
action.}\label{discrete-action-spaces-the-neural-network-has-one-node-for-each-possible-action.}}

\hypertarget{continuous-action-spaces-the-neural-network-has-one-node-for-each-action-entry-or-index.}{%
\subsection{* Continuous Action spaces : the neural network has one node
for each action entry (or
index).}\label{continuous-action-spaces-the-neural-network-has-one-node-for-each-action-entry-or-index.}}

    \hypertarget{in-the-bipedalwalker-httpsgithub.comopenaigymwikibipedalwalker-v2-case}{%
\subsubsection{In the {[}BipedalWalker{]}
\{https://github.com/openai/gym/wiki/BipedalWalker-v2\}
case}\label{in-the-bipedalwalker-httpsgithub.comopenaigymwikibipedalwalker-v2-case}}

\hypertarget{any-action-is-a-vector-of-four-numbers-so-the-output-layer-of-the-policy-network-will-have-four-nodes.}{%
\subsection{Any action is a vector of four numbers, so the output layer
of the policy network will have four
nodes.}\label{any-action-is-a-vector-of-four-numbers-so-the-output-layer-of-the-policy-network-will-have-four-nodes.}}

\hypertarget{since-every-entry-in-the-action-must-be-a-number-between--1-and-1-we-will-add-a-tanh-activation-function-httpspytorch.orgdocsstablenn.htmltorch.nn.tanh-to-the-output-layer}{%
\paragraph{Since every entry in the action must be a number between -1
and 1, we will add a {[}tanh activation function{]}
\{https://pytorch.org/docs/stable/nn.html\#torch.nn.Tanh\} to the output
layer}\label{since-every-entry-in-the-action-must-be-a-number-between--1-and-1-we-will-add-a-tanh-activation-function-httpspytorch.orgdocsstablenn.htmltorch.nn.tanh-to-the-output-layer}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}3\PYZhy{}3\PYZus{}continuous\PYZus{}action\PYZus{}space\PYZus{}in\PYZus{}mountain\PYZus{}car\PYZus{}continuous.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}12}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_27_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{in-the-mountaincarcontinuous-httpsgithub.comopenaigymwikimountaincarcontinuous-v0-case}{%
\subsubsection{In the {[}MountainCarContinuous{]}
\{https://github.com/openai/gym/wiki/MountainCarContinuous-v0\}
case}\label{in-the-mountaincarcontinuous-httpsgithub.comopenaigymwikimountaincarcontinuous-v0-case}}

\hypertarget{the-action-space-is-shown-in-the-figure-above.-note-that-for-this-environment-the-action-must-be-a-value-between--1-and-1}{%
\subsection{The action space is shown in the figure above. Note that for
this environment, the action must be a value between -1 and
1}\label{the-action-space-is-shown-in-the-figure-above.-note-that-for-this-environment-the-action-must-be-a-value-between--1-and-1}}

    \hypertarget{quiz}{%
\subsection{Quiz}\label{quiz}}

\hypertarget{consider-the-mountaincarcontinuous-v0-environment.-which-of-the-following-describes-a-valid-output-layer-for-the-policy-select-the-option-that-yields-valid-actions-that-can-be-passed-directly-to-the-environment-without-any-additional-preprocessing.}{%
\subsubsection{Consider the MountainCarContinuous-v0 environment. Which
of the following describes a valid output layer for the policy? (Select
the option that yields valid actions that can be passed directly to the
environment without any additional
preprocessing.)}\label{consider-the-mountaincarcontinuous-v0-environment.-which-of-the-following-describes-a-valid-output-layer-for-the-policy-select-the-option-that-yields-valid-actions-that-can-be-passed-directly-to-the-environment-without-any-additional-preprocessing.}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Layer size:1, Activation fuction: softmax
\item
  Layer size:1, Activation fuction: tanh
\item
  Layer size:2, Activation fuction: softmax
\item
  Layer size:2, Activation fuction: ReLu
\end{enumerate}

    \hypertarget{hill-climbing-algorithm}{%
\subsection{3-1-4 : Hill Climbing
Algorithm}\label{hill-climbing-algorithm}}

{[}hill climbing{]} \{https://en.wikipedia.org/wiki/Hill\_climbing\} is
not just for reinforcement learning! It is a general optimization method
that is used to find the maximum of a function.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}4\PYZhy{}1\PYZus{}hill\PYZus{}climbiing\PYZus{}neural\PYZus{}network\PYZus{}input\PYZus{}and\PYZus{}output.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}13}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_31_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}4\PYZhy{}2\PYZus{}hill\PYZus{}climbiing\PYZus{}neural\PYZus{}relation\PYZus{}between\PYZus{}j\PYZus{}theta.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}14}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_32_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{well-refore-to-the-set-of-weights-in-the-neural-network-as-theta.}{%
\paragraph{We'll refore to the set of weights in the neural network as
Theta.}\label{well-refore-to-the-set-of-weights-in-the-neural-network-as-theta.}}

\hypertarget{theres-some-mathematical-relationship-between-theta-and-the-expected-return-j.}{%
\paragraph{There's some mathematical relationship between Theta and the
expected return
J.}\label{theres-some-mathematical-relationship-between-theta-and-the-expected-return-j.}}

\hypertarget{main-idea-is-that-its-possible-to-write-the-expected-return-j-as-a-fuction-of-theta.}{%
\subsubsection{Main idea is that it's possible to write the expected
return J as a fuction of
Theta.}\label{main-idea-is-that-its-possible-to-write-the-expected-return-j-as-a-fuction-of-theta.}}

\hypertarget{our-goal-is-to-find-the-values-for-theta.}{%
\subsection{Our goal is to find the values for
Theta.}\label{our-goal-is-to-find-the-values-for-theta.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}4\PYZhy{}3\PYZus{}hill\PYZus{}climbiing\PYZus{}neural\PYZus{}gradient\PYZus{}ascent.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}15}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_34_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{gradient-ascent}{%
\subsection{Gradient Ascent}\label{gradient-ascent}}

Gradient ascent is similar to gradient descent.

\begin{verbatim}
* Gradient descent steps in the direction opposite the gradient, since it wants to minimize a function.
* Gradient ascent is otherwise identical, except we step in the direction of the gradient, to reach the maximum.
\end{verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}4\PYZhy{}4\PYZus{}hill\PYZus{}climbiing\PYZus{}reach\PYZus{}maximum\PYZus{}value\PYZus{}of\PYZus{}function.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}16}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_36_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}4\PYZhy{}5\PYZus{}hill\PYZus{}climbiing\PYZus{}reach\PYZus{}optimal\PYZus{}value.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}17}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{local-minima}{%
\subsection{Local Minima}\label{local-minima}}

\hypertarget{hill-climbing-is-a-relatively-simple-algorithm-that-the-agent-can-use-to-gradually-improve-the-weights-ux3b8-in-its-policy-network-while-interacting-with-the-environment.}{%
\subsubsection{* hill climbing is a relatively simple algorithm that the
agent can use to gradually improve the weights θ in its policy network
while interacting with the
environment.}\label{hill-climbing-is-a-relatively-simple-algorithm-that-the-agent-can-use-to-gradually-improve-the-weights-ux3b8-in-its-policy-network-while-interacting-with-the-environment.}}

\hypertarget{hill-climbing-is-not-guaranteed-to-always-yield-the-weights-of-the-optimal-policy.-this-is-because-we-can-easily-get-stuck-in-a-local-maximum.}{%
\subsubsection{* hill climbing is not guaranteed to always yield the
weights of the optimal policy. This is because we can easily get stuck
in a local
maximum.}\label{hill-climbing-is-not-guaranteed-to-always-yield-the-weights-of-the-optimal-policy.-this-is-because-we-can-easily-get-stuck-in-a-local-maximum.}}

    \hypertarget{hill-climbing-pseudocode}{%
\subsection{3-1-5 : Hill Climbing
Pseudocode}\label{hill-climbing-pseudocode}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}5\PYZhy{}1\PYZus{}hill\PYZus{}climbiing\PYZus{}pseudocode.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}18}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_40_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{g-vs.j-whats-the-difference}{%
\subsection{G vs.~J (What's the
difference?)}\label{g-vs.j-whats-the-difference}}

\hypertarget{the-agent-collects-in-a-single-episode-g-from-the-pseudocode-above-and-the-expected-return-j.}{%
\paragraph{The agent collects in a single episode (G from the pseudocode
above) and the expected return
J.}\label{the-agent-collects-in-a-single-episode-g-from-the-pseudocode-above-and-the-expected-return-j.}}

\hypertarget{in-reinforcement-learning-the-goal-of-the-agent-is-to-find-the-value-of-the-policy-network-weights-ux3b8-that-maximizes-expected-return-which-we-have-denoted-by-j.}{%
\paragraph{In reinforcement learning, the goal of the agent is to find
the value of the policy network weights θ that maximizes {[}expected{]}
return, which we have denoted by
J.}\label{in-reinforcement-learning-the-goal-of-the-agent-is-to-find-the-value-of-the-policy-network-weights-ux3b8-that-maximizes-expected-return-which-we-have-denoted-by-j.}}

\hypertarget{in-the-hill-climbing-algorithm-the-values-of-ux3b8-are-evaluated-according-to-how-much-return-g-they-collected-in-a-single-episode}{%
\subsubsection{In the hill climbing algorithm, the values of θ are
evaluated according to how much return G they collected in a {[}single
episode{]}}\label{in-the-hill-climbing-algorithm-the-values-of-ux3b8-are-evaluated-according-to-how-much-return-g-they-collected-in-a-single-episode}}

\hypertarget{due-to-randomness-in-the-environment-and-the-policy-if-it-is-stochastic-it-is-highly-likely-that-if-we-collect-a-second-episode-with-the-same-values-for-ux3b8-well-likely-get-a-different-value-for-the-return-g.}{%
\subsection{Due to randomness in the environment (and the policy, if it
is stochastic), it is highly likely that if we collect a second episode
with the same values for θ, we'll likely get a different value for the
return
G.}\label{due-to-randomness-in-the-environment-and-the-policy-if-it-is-stochastic-it-is-highly-likely-that-if-we-collect-a-second-episode-with-the-same-values-for-ux3b8-well-likely-get-a-different-value-for-the-return-g.}}

\hypertarget{because-of-this-the-sampled-return-g-is-not-a-perfect-estimate-for-the-expected-return-j-but-it-often-turns-out-to-be-good-enough-in-practic}{%
\section{Because of this, the (sampled) return G is not a perfect
estimate for the expected return J, but it often turns out to be {[}good
enough{]} in
practic}\label{because-of-this-the-sampled-return-g-is-not-a-perfect-estimate-for-the-expected-return-j-but-it-often-turns-out-to-be-good-enough-in-practic}}

    \hypertarget{beyond-hill-climbing}{%
\subsection{3-1-6 : Beyond Hill Climbing}\label{beyond-hill-climbing}}

\begin{itemize}
\tightlist
\item
  We refer to the general class of approaches that find argmax J(θ)
  through randomly perturbing the most recent best estimate as
  {[}stochastic policy search{]}.
\item
  Likewise, we can refer to J as an {[}objective function{]}, which just
  refers to the fact that we'd like to maximize it.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}6\PYZhy{}1\PYZus{}hill\PYZus{}climbiing\PYZus{}we\PYZus{}dont\PYZus{}know\PYZus{}j.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}19}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_43_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}6\PYZhy{}2\PYZus{}hill\PYZus{}climbiing\PYZus{}with\PYZus{}sochastic\PYZus{}policy\PYZus{}search\PYZus{}returns\PYZus{}objective\PYZus{}value.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}20}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_44_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}6\PYZhy{}3\PYZus{}hill\PYZus{}climbiing\PYZus{}policy\PYZus{}is\PYZus{}somewhere\PYZus{}on\PYZus{}the\PYZus{}objective\PYZus{}function\PYZus{}surface.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}21}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_45_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}6\PYZhy{}4\PYZus{}hill\PYZus{}climbiing\PYZus{}change\PYZus{}parameters\PYZus{}by\PYZus{}adding\PYZus{}gaussian\PYZus{}noise.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}22}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_46_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}6\PYZhy{}5\PYZus{}hill\PYZus{}climbiing\PYZus{}set\PYZus{}this\PYZus{}policy\PYZus{}to\PYZus{}new\PYZus{}best\PYZus{}policy.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}23}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_47_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}6\PYZhy{}6\PYZus{}hill\PYZus{}climbiing\PYZus{}iterate\PYZus{}until\PYZus{}top\PYZus{}of\PYZus{}the\PYZus{}hill.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}24}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_48_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{the-best-part-of-hill-climbing-is-that-you-can-use-any-policy-function.}{%
\subsection{The best part of `Hill Climbing' is that ``You can use any
policy
function''.}\label{the-best-part-of-hill-climbing-is-that-you-can-use-any-policy-function.}}

\hypertarget{it-doesnt-need-to-be-differentiable-or-even-continuous.}{%
\subsection{It doesn't need to be differentiable or even
continuous.}\label{it-doesnt-need-to-be-differentiable-or-even-continuous.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}6\PYZhy{}7\PYZus{}beyond\PYZus{}hill\PYZus{}climbiing\PYZus{}steepest\PYZus{}ascent.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}25}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_50_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{stepest-ascent}{%
\subsection{Stepest Ascent}\label{stepest-ascent}}

\begin{itemize}
\tightlist
\item
  It generates several neighboring policies at each iteration.
\item
  It helps reduce the risk of selecting a next policy that may lead to a
  suboptimal solution.
\item
  You could still get stuck in local optima.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}6\PYZhy{}8\PYZus{}beyond\PYZus{}hill\PYZus{}climbiing\PYZus{}simulated\PYZus{}annealing.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}26}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_52_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}6\PYZhy{}9\PYZus{}beyond\PYZus{}hill\PYZus{}climbiing\PYZus{}adaptive\PYZus{}noise\PYZus{}same\PYZus{}as\PYZus{}sa.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}27}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_53_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}6\PYZhy{}10\PYZus{}beyond\PYZus{}hill\PYZus{}climbiing\PYZus{}adaptive\PYZus{}noise\PYZus{}extend\PYZus{}serach\PYZus{}radius\PYZus{}when\PYZus{}is\PYZus{}not\PYZus{}best.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}28}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_54_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{more-black-box-optimization}{%
\subsection{3-1-7 : More Black-Box
Optimization}\label{more-black-box-optimization}}

\hypertarget{black-box-refers-to-the-fact-that-in-order-to-find-the-value-of-ux3b8-that-maximizes-the-function-jjux3b8-we-need-only-be-able-to-estimate-the-value-of-j-at-any-potential-value-of-ux3b8.}{%
\paragraph{* Black-box refers to the fact that in order to find the
value of θ that maximizes the function J=J(θ), we need only be able to
estimate the value of J at any potential value of
θ.}\label{black-box-refers-to-the-fact-that-in-order-to-find-the-value-of-ux3b8-that-maximizes-the-function-jjux3b8-we-need-only-be-able-to-estimate-the-value-of-j-at-any-potential-value-of-ux3b8.}}

\hypertarget{both-hill-climbing-and-steepest-ascent-hill-climbing-dont-know-that-were-solving-a-reinforcement-learning-problem-and-they-do-not-care-that-the-function-were-trying-to-maximize-corresponds-to-the-expected-return.}{%
\paragraph{* Both hill climbing and steepest ascent hill climbing don't
know that we're solving a reinforcement learning problem, and they do
not care that the function we're trying to maximize corresponds to the
expected
return.}\label{both-hill-climbing-and-steepest-ascent-hill-climbing-dont-know-that-were-solving-a-reinforcement-learning-problem-and-they-do-not-care-that-the-function-were-trying-to-maximize-corresponds-to-the-expected-return.}}

\hypertarget{these-algorithms-only-know-that-for-each-value-of-ux3b8-theres-a-corresponding-number.}{%
\subsubsection{* These algorithms only know that for each value of θ,
there's a corresponding
number.}\label{these-algorithms-only-know-that-for-each-value-of-ux3b8-theres-a-corresponding-number.}}

\hypertarget{we-know-that-this-number-corresponds-to-the-return-obtained-by-using-the-policy-corresponding-to-ux3b8-to-collect-an-episode-but-the-algorithms-are-not-aware-of-this.}{%
\subsubsection{* We know that this number corresponds to the return
obtained by using the policy corresponding to θ to collect an episode,
but the algorithms are not aware of
this.}\label{we-know-that-this-number-corresponds-to-the-return-obtained-by-using-the-policy-corresponding-to-ux3b8-to-collect-an-episode-but-the-algorithms-are-not-aware-of-this.}}

\hypertarget{to-the-algorithms-the-way-we-evaluate-ux3b8-is-considered-a-black-box-and-they-dont-worry-about-the-details.}{%
\subsection{To the algorithms, the way we evaluate θ is considered a
black box, and they don't worry about the
details.}\label{to-the-algorithms-the-way-we-evaluate-ux3b8-is-considered-a-black-box-and-they-dont-worry-about-the-details.}}

\hypertarget{the-algorithms-only-care-about-finding-the-value-of-ux3b8-that-will-maximize-the-number-that-comes-out-of-the-black-box.}{%
\subsection{The algorithms only care about finding the value of θ that
will maximize the number that comes out of the black
box.}\label{the-algorithms-only-care-about-finding-the-value-of-ux3b8-that-will-maximize-the-number-that-comes-out-of-the-black-box.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}7\PYZhy{}1\PYZus{}steepest\PYZus{}ascent\PYZus{}dont\PYZus{}use\PYZus{}usuful\PYZus{}imformation\PYZus{}from\PYZus{}not\PYZus{}selected.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}29}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_56_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}7\PYZhy{}2\PYZus{}cross\PYZus{}entropy\PYZus{}method\PYZus{}select\PYZus{}top\PYZus{}n\PYZus{}and\PYZus{}use\PYZus{}average\PYZus{}of\PYZus{}them.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}30}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_57_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}7\PYZhy{}3\PYZus{}evolution\PYZus{}strategies\PYZus{}the\PYZus{}best\PYZus{}policy\PYZus{}is\PYZus{}weighted\PYZus{}sum\PYZus{}of\PYZus{}all\PYZus{}selected.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}31}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_58_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{why-policy-based-methods}{%
\subsection{3-1-8 : Why Policy-Based
Methods?}\label{why-policy-based-methods}}

why do we need policy-based methods at all, when value-based methods
work so well?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}8\PYZhy{}1\PYZus{}policy\PYZus{}based\PYZus{}method\PYZus{}why.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}32}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_60_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}8\PYZhy{}2\PYZus{}policy\PYZus{}based\PYZus{}method\PYZus{}simplicity.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}33}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_61_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}8\PYZhy{}3\PYZus{}policy\PYZus{}based\PYZus{}method\PYZus{}policy\PYZus{}look\PYZus{}like.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}34}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_62_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{itemize}
\tightlist
\item
  Deterministic Policy : Simply need to be a mapping or fuction from
  states to actions.
\item
  Stochastic Policy : We woul choose an action based on this probability
  distribution.
\end{itemize}

\hypertarget{advantages-of-directly-estimating-the-optimal-policy-policy-based-methods-main-idea}{%
\subsection{Advantages of ``Directly Estimating the optimal policy =
Policy-based method's main
idea''}\label{advantages-of-directly-estimating-the-optimal-policy-policy-based-methods-main-idea}}

\hypertarget{it-avoids-having-to-store-a-bunch-of-additional-data.large-portions-of-the-state-space-may-have-the-same-value.}{%
\subsubsection{1 it avoids having to store a bunch of additional
data.(large portions of the state space may have the same
value.)}\label{it-avoids-having-to-store-a-bunch-of-additional-data.large-portions-of-the-state-space-may-have-the-same-value.}}

\hypertarget{it-is-easy-to-make-algorithm-more-generalized.}{%
\subsubsection{2 it is easy to make algorithm more
generalized.}\label{it-is-easy-to-make-algorithm-more-generalized.}}

\hypertarget{we-can-focus-more-on-the-complicated-regions-of-state-space.}{%
\subsubsection{3 we can focus more on the complicated regions of state
space.}\label{we-can-focus-more-on-the-complicated-regions-of-state-space.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}8\PYZhy{}4\PYZus{}policy\PYZus{}based\PYZus{}method\PYZus{}stochastic\PYZus{}plicies.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}35}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_64_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{one-of-the-main-advantages-of-policy-based-methods-over-value-based-methods-is-that-policy-based-methods-can-learn-true-stochastic-policies}{%
\subsection{One of the main advantages of policy-based methods over
value-based methods is that ``Policy-based methods can learn true
stochastic
policies''}\label{one-of-the-main-advantages-of-policy-based-methods-over-value-based-methods-is-that-policy-based-methods-can-learn-true-stochastic-policies}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}8\PYZhy{}5\PYZus{}value\PYZus{}based\PYZus{}method\PYZus{}e\PYZhy{}greedy\PYZus{}is\PYZus{}a\PYZus{}hack.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}36}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{in-contrast-when-we-apply-epsilon-greedy-actions-selection-to-a-value-fuction-that-does-add-some-randomness-but-it-is-a-hack.}{%
\subsection{In contrast, when we apply epsilon-greedy actions selection
to a value fuction, that does add some randomness, but it is a
hack.}\label{in-contrast-when-we-apply-epsilon-greedy-actions-selection-to-a-value-fuction-that-does-add-some-randomness-but-it-is-a-hack.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}8\PYZhy{}6\PYZus{}policy\PYZus{}based\PYZus{}method\PYZus{}aliased\PYZus{}states.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}37}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_68_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}8\PYZhy{}7\PYZus{}policy\PYZus{}based\PYZus{}method\PYZus{}aliased\PYZus{}states\PYZus{}if\PYZus{}using\PYZus{}value\PYZus{}function.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}38}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_69_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}8\PYZhy{}8\PYZus{}policy\PYZus{}based\PYZus{}method\PYZus{}aliased\PYZus{}states\PYZus{}if\PYZus{}using\PYZus{}value\PYZus{}function\PYZus{}then\PYZus{}keep\PYZus{}oscillating\PYZus{}never\PYZus{}get\PYZus{}out.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}39}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_70_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}8\PYZhy{}9\PYZus{}policy\PYZus{}based\PYZus{}method\PYZus{}aliased\PYZus{}states\PYZus{}if\PYZus{}using\PYZus{}policy\PYZus{}based\PYZus{}method\PYZus{}then\PYZus{}learn\PYZus{}desired\PYZus{}stochastic\PYZus{}policy.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}40}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_71_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}8\PYZhy{}10\PYZus{}policy\PYZus{}based\PYZus{}method\PYZus{}discret\PYZus{}action\PYZus{}space.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}41}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_72_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}8\PYZhy{}11\PYZus{}policy\PYZus{}based\PYZus{}method\PYZus{}continuous\PYZus{}action\PYZus{}space.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}42}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_73_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}1\PYZhy{}8\PYZhy{}12\PYZus{}policy\PYZus{}based\PYZus{}method\PYZus{}high\PYZhy{}dimensional\PYZus{}continuous\PYZus{}action\PYZus{}space.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}43}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_74_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{lesson-3-2-policy-gradient-methods}{%
\section{Lesson 3-2: Policy Gradient
Methods}\label{lesson-3-2-policy-gradient-methods}}

    \hypertarget{what-are-policy-gradient-methods}{%
\subsection{3-2-1 : What are Policy Gradient
Methods?}\label{what-are-policy-gradient-methods}}

Policy gradient methods are a subclass of policy-based methods.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}1\PYZhy{}1\PYZus{}policy\PYZus{}gradient\PYZus{}methods\PYZus{}is\PYZus{}subset\PYZus{}of\PYZus{}policy\PYZus{}based\PYZus{}methods.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}44}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_77_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}1\PYZhy{}2\PYZus{}policy\PYZus{}gradient\PYZus{}methods\PYZus{}chicken\PYZus{}cross\PYZus{}the\PYZus{}road.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}45}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_78_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}1\PYZhy{}3\PYZus{}policy\PYZus{}gradient\PYZus{}methods\PYZus{}case\PYZus{}of\PYZus{}four\PYZus{}possible\PYZus{}actions.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}46}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_79_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}1\PYZhy{}4\PYZus{}policy\PYZus{}gradient\PYZus{}methods\PYZus{}possibly\PYZus{}cnn\PYZus{}is\PYZus{}best.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}47}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_80_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}1\PYZhy{}5\PYZus{}policy\PYZus{}gradient\PYZus{}methods\PYZus{}reward\PYZus{}only\PYZus{}delivered\PYZus{}at\PYZus{}the\PYZus{}end\PYZus{}of\PYZus{}game.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}48}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_81_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{the-big-picture-of-pg}{%
\subsection{3-2-2 : The Big Picture of PG}\label{the-big-picture-of-pg}}

Before digging into the details of policy gradient methods, we'll
discuss how they work at a high level.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}2\PYZhy{}1\PYZus{}pg\PYZus{}big\PYZus{}picture\PYZus{}case\PYZus{}of\PYZus{}win.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}49}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_83_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}2\PYZhy{}2\PYZus{}pg\PYZus{}big\PYZus{}picture\PYZus{}case\PYZus{}of\PYZus{}win\PYZus{}getting\PYZus{}action\PYZus{}posibilities\PYZus{}from\PYZus{}1st\PYZus{}timestamp.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}50}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_84_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}2\PYZhy{}3\PYZus{}pg\PYZus{}big\PYZus{}picture\PYZus{}case\PYZus{}of\PYZus{}win\PYZus{}change\PYZus{}parameters\PYZus{}a\PYZus{}litte\PYZus{}bit\PYZus{}to\PYZus{}direction\PYZus{}winning\PYZus{}game.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}51}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_85_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}2\PYZhy{}4\PYZus{}pg\PYZus{}big\PYZus{}picture\PYZus{}case\PYZus{}of\PYZus{}win\PYZus{}getting\PYZus{}action\PYZus{}posibilities\PYZus{}from\PYZus{}2nd\PYZus{}timestamp.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}52}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_86_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}2\PYZhy{}5\PYZus{}pg\PYZus{}big\PYZus{}picture\PYZus{}case\PYZus{}of\PYZus{}win\PYZus{}change\PYZus{}parameters\PYZus{}a\PYZus{}litte\PYZus{}bit\PYZus{}to\PYZus{}up\PYZus{}direction\PYZus{}winning\PYZus{}game.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}53}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_87_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}54}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}2\PYZhy{}6\PYZus{}pg\PYZus{}big\PYZus{}picture\PYZus{}case\PYZus{}of\PYZus{}lost.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}54}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_88_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}55}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}2\PYZhy{}7\PYZus{}pg\PYZus{}big\PYZus{}picture\PYZus{}case\PYZus{}of\PYZus{}lost\PYZus{}getting\PYZus{}action\PYZus{}posibilities\PYZus{}from\PYZus{}1st\PYZus{}timestamp.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}55}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_89_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}56}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}2\PYZhy{}8\PYZus{}pg\PYZus{}big\PYZus{}picture\PYZus{}case\PYZus{}of\PYZus{}lost\PYZus{}change\PYZus{}parameters\PYZus{}a\PYZus{}litte\PYZus{}bit\PYZus{}to\PYZus{}direction\PYZus{}winning\PYZus{}game.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}56}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_90_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}57}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}2\PYZhy{}9\PYZus{}pg\PYZus{}big\PYZus{}picture\PYZus{}pseudocode.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}57}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_91_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{connections-to-supervised-learning}{%
\subsection{3-2-3 : Connections to Supervised
Learning}\label{connections-to-supervised-learning}}

Policy gradient methods are very similar to supervised learning.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}58}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}3\PYZhy{}1\PYZus{}pg\PYZus{}is\PYZus{}similar\PYZus{}to\PYZus{}supervised\PYZus{}learning.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}58}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_93_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{difference-between-rlpg-and-sl}{%
\subsection{Difference between RL(PG) and
SL}\label{difference-between-rlpg-and-sl}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In SL, we typically work with the dataset that doesn't change over
  time.
\item
  In RL, the dataset varies by episode and changes pretty frequently.
\item
  In both(SL,RL), the dataset has mutiple conflicting opinions about
  what the best output should be for an input.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}59}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}3\PYZhy{}2\PYZus{}pg\PYZus{}is\PYZus{}similar\PYZus{}to\PYZus{}sl\PYZus{}mutiple\PYZus{}conflicting\PYZus{}opinions.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}59}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_95_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}60}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}3\PYZhy{}3\PYZus{}more\PYZus{}learn\PYZus{}connection\PYZus{}between\PYZus{}sl\PYZus{}with\PYZus{}rl.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}60}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_96_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{to-further-explore-the-connections-between-policy-gradient-methods-and-supervised-learning-youre-encouraged-to-check-out-andrej-karpathys-famous-blog-post-httpkarpathy.github.io20160531rl.}{%
\subsubsection{To further explore the connections between policy
gradient methods and supervised learning, you're encouraged to check out
{[}Andrej Karpathy's famous blog post{]}
\{http://karpathy.github.io/2016/05/31/rl/\}.}\label{to-further-explore-the-connections-between-policy-gradient-methods-and-supervised-learning-youre-encouraged-to-check-out-andrej-karpathys-famous-blog-post-httpkarpathy.github.io20160531rl.}}

    \hypertarget{how-pg-work}{%
\subsection{3-2-4 : How PG work?}\label{how-pg-work}}

Define how policy gradient methods will work.

    \hypertarget{why-trajectories}{%
\subsection{Why Trajectories?}\label{why-trajectories}}

\hypertarget{question-isyou-may-be-wondering-why-are-we-using-trajectories-instead-of-episodes}{%
\subsubsection{Question is\ldots{}``You may be wondering: why are we
using trajectories instead of
episodes?''}\label{question-isyou-may-be-wondering-why-are-we-using-trajectories-instead-of-episodes}}

\hypertarget{answer-ismaximizing-expected-return-over-trajectories-instead-of-episodes-lets-us-search-for-optimal-policies-for-both-episodic-and-continuing-tasks}{%
\subsubsection{Answer is\ldots{}``maximizing expected return over
trajectories (instead of episodes) lets us search for optimal policies
for both episodic and continuing
tasks!''}\label{answer-ismaximizing-expected-return-over-trajectories-instead-of-episodes-lets-us-search-for-optimal-policies-for-both-episodic-and-continuing-tasks}}

\begin{itemize}
\tightlist
\item
  For many episodic tasks, it often makes sense to just use the full
  episode. In particular, for the case of the video game examples,
  reward is only delivered at the end of the episode.
\item
  In order to estimate the expected return, the trajectory should
  correspond to the full episode; otherwise, we don't have enough reward
  information to meaningfully estimate the expected return.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}61}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}4\PYZhy{}1\PYZus{}how\PYZus{}pg\PYZus{}work\PYZus{}trajectory\PYZus{}is\PYZus{}state\PYZus{}action\PYZus{}sequence.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}61}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_100_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{notation}{%
\subsection{Notation}\label{notation}}

\hypertarget{we-denote-the-trajectory-as-tau.}{%
\subsubsection{* we denote ``the Trajectory'' as
Tau.}\label{we-denote-the-trajectory-as-tau.}}

\hypertarget{we-denote-the-sum-reward-from-that-trajectory-as-r-of-tau.}{%
\subsubsection{* we denote ``the sum reward from that Trajectory'' as R
of
Tau.}\label{we-denote-the-sum-reward-from-that-trajectory-as-r-of-tau.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}62}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}4\PYZhy{}2\PYZus{}how\PYZus{}pg\PYZus{}work\PYZus{}trajectory\PYZus{}does\PYZus{}not\PYZus{}keep\PYZus{}track\PYZus{}rewards.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}62}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_102_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{our-goal-is-finding-the-weight-theta-of-the-neural-network-that-maximize-expected-return.}{%
\subsection{Our goal is finding the weight Theta of the neural network
that maximize expected
return.}\label{our-goal-is-finding-the-weight-theta-of-the-neural-network-that-maximize-expected-return.}}

\hypertarget{one-way-of-accomplishing-our-goal-is-by-setting-the-weights-of-neural-network-so-that-on-average.}{%
\subsection{One way of accomplishing our goal is by setting the weights
of neural network so that on
average.}\label{one-way-of-accomplishing-our-goal-is-by-setting-the-weights-of-neural-network-so-that-on-average.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}63}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}4\PYZhy{}3\PYZus{}how\PYZus{}pg\PYZus{}work\PYZus{}deep\PYZus{}dive.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}63}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_104_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{notation}{%
\subsection{Notation}\label{notation}}

\hypertarget{we-denote-the-expected-return-by-u.}{%
\subsubsection{* We denote ``the expected return'' by
U.}\label{we-denote-the-expected-return-by-u.}}

\hypertarget{u-is-a-fuction-of-theta.}{%
\subsubsection{* U is a fuction of
Theta.}\label{u-is-a-fuction-of-theta.}}

\hypertarget{we-want-to-maximize-theta.}{%
\subsubsection{* We want to maximize
Theta.}\label{we-want-to-maximize-theta.}}

\hypertarget{the-notation-semicolon-means-that-theta-has-influence-on-the-probability-of-a-trajectory.}{%
\subsubsection{* The notation ``Semicolon'' means that Theta has
influence on the probability of a
Trajectory.}\label{the-notation-semicolon-means-that-theta-has-influence-on-the-probability-of-a-trajectory.}}

\hypertarget{pg-working-explanation}{%
\subsection{PG working explanation}\label{pg-working-explanation}}

\hypertarget{r-of-tau-is-just-the-return-corresponding-to-an-arbitrary-trajectory.}{%
\subsubsection{1. ``R of Tau'' is just the return corresponding to an
arbitrary
Trajectory.}\label{r-of-tau-is-just-the-return-corresponding-to-an-arbitrary-trajectory.}}

\hypertarget{take-this-value-of-r-of-tau-and-use-it-to-calculate-the-expected-return-u.}{%
\subsubsection{2. Take this value of ``R of Tau'' and use it to
calculate the expected return
``U''.}\label{take-this-value-of-r-of-tau-and-use-it-to-calculate-the-expected-return-u.}}

\hypertarget{we-need-only-take-into-account-the-probability-of-each-possible-trajectory.}{%
\subsubsection{3. We need only take into account the probability of each
possible
``Trajectory''.}\label{we-need-only-take-into-account-the-probability-of-each-possible-trajectory.}}

\hypertarget{that-probability-depends-on-the-weights-theta-in-the-neural-network.}{%
\subsubsection{4. That probability depends on the weights ``Theta'' in
the neural
network.}\label{that-probability-depends-on-the-weights-theta-in-the-neural-network.}}

\hypertarget{theta-defines-the-policy.}{%
\subsubsection{5. ``Theta'' defines the
policy.}\label{theta-defines-the-policy.}}

\hypertarget{policy-is-used-to-select-the-actions-in-the-trajectory.}{%
\subsubsection{6. Policy is used to select the actions in the
Trajectory.}\label{policy-is-used-to-select-the-actions-in-the-trajectory.}}

\hypertarget{actions-is-determining-the-states-in-the-trajectory.}{%
\subsubsection{7. Actions is determining the states in the
Trajectory.}\label{actions-is-determining-the-states-in-the-trajectory.}}

    \hypertarget{reinforce}{%
\subsection{3-2-5 : REINFORCE}\label{reinforce}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}64}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}5\PYZhy{}1\PYZus{}reinforce\PYZus{}check\PYZus{}our\PYZus{}goal\PYZus{}first.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}64}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_107_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{our-goal-is-finding-the-weight-theta-of-the-neural-network-that-maximize-expected-return.}{%
\subsection{Our goal is finding the weight Theta of the neural network
that maximize expected
return.}\label{our-goal-is-finding-the-weight-theta-of-the-neural-network-that-maximize-expected-return.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}65}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}5\PYZhy{}2\PYZus{}reinforce\PYZus{}one\PYZus{}way\PYZus{}to\PYZus{}achieve\PYZus{}this\PYZus{}goal\PYZus{}is\PYZus{}gradient\PYZus{}ascent.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}65}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_109_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{one-way-to-do-that-is-by-gradient-ascent-where-we-just-iteratively-take-small-steps-in-the-direction-of-the-gradient.}{%
\subsection{One way to do that is by Gradient Ascent, where we just
iteratively take small steps in the direction of the
gradient.}\label{one-way-to-do-that-is-by-gradient-ascent-where-we-just-iteratively-take-small-steps-in-the-direction-of-the-gradient.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}66}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}5\PYZhy{}3\PYZus{}reinforce\PYZus{}ga\PYZus{}vs\PYZus{}gd.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}66}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_111_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{remember-that-alpha-is-the-step-size-and-let-it-decay-over-time-to-avoid-overshooting-the-target.}{%
\subsection{Remember that alpha is the step size and let it decay over
time to avoid overshooting the
target.}\label{remember-that-alpha-is-the-step-size-and-let-it-decay-over-time-to-avoid-overshooting-the-target.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}67}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}5\PYZhy{}4\PYZus{}reinforce\PYZus{}calculating\PYZus{}gradient\PYZus{}is\PYZus{}very\PYZus{}expensive.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}67}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_113_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{we-wont-be-able-to-calculate-the-exact-value-of-the-gradient-since-that-is-computationally-too-expensive.}{%
\subsection{We won't be able to calculate the exact value of the
gradient since that is computationally too
expensive.}\label{we-wont-be-able-to-calculate-the-exact-value-of-the-gradient-since-that-is-computationally-too-expensive.}}

\hypertarget{well-have-to-consider-every-possible-trajectory.}{%
\subsection{We'll have to consider every possible
trajectory.}\label{well-have-to-consider-every-possible-trajectory.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}68}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}5\PYZhy{}5\PYZus{}reinforce\PYZus{}estimate\PYZus{}gradient\PYZus{}and\PYZus{}consider\PYZus{}a\PYZus{}few\PYZus{}trajectories.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}68}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_115_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{notation}{%
\subsection{Notation}\label{notation}}

\hypertarget{we-only-consider-a-few-trajectories.-so-we-denote-those-by-tau-1-tau-2-tau-m.}{%
\subsubsection{* We only consider a few trajectories. so we denote those
by ``Tau 1'', ``Tau 2'', \ldots{}, ``Tau
M''.}\label{we-only-consider-a-few-trajectories.-so-we-denote-those-by-tau-1-tau-2-tau-m.}}

\hypertarget{remember-that-any-trajectory-is-just-a-sequence-of-states-and-actions.}{%
\subsubsection{* Remember that any trajectory is just a sequence of
states and
actions.}\label{remember-that-any-trajectory-is-just-a-sequence-of-states-and-actions.}}

\hypertarget{we-denote-estimated-value-from-the-m-trajectories-by-g-hat.}{%
\subsubsection{* We denote estimated value from the ``M trajectories''
by ``g
hat''.}\label{we-denote-estimated-value-from-the-m-trajectories-by-g-hat.}}

\hypertarget{once-we-have-an-estimate-for-the-gradient-we-can-use-it-to-update-the-weights-of-the-policy.}{%
\subsection{Once we have an estimate for the gradient, we can use it to
update the weights of the
policy.}\label{once-we-have-an-estimate-for-the-gradient-we-can-use-it-to-update-the-weights-of-the-policy.}}

\hypertarget{then-we-repeatdly-loop-over-these-steps-to-converge-to-the-weights-of-the-optimal-policy.}{%
\subsection{Then, we repeatdly loop over these steps to converge to the
weights of the optimal
policy.}\label{then-we-repeatdly-loop-over-these-steps-to-converge-to-the-weights-of-the-optimal-policy.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}69}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}5\PYZhy{}6\PYZus{}reinforce\PYZus{}what\PYZus{}if\PYZus{}m\PYZus{}equals\PYZus{}one\PYZus{}trajectory.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}69}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_117_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}70}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}5\PYZhy{}7\PYZus{}reinforce\PYZus{}detailed\PYZus{}meaning\PYZus{}when\PYZus{}one\PYZus{}trajectory.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}70}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_118_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}71}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}5\PYZhy{}8\PYZus{}reinforce\PYZus{}detailed\PYZus{}meaning\PYZus{}direction\PYZus{}of\PYZus{}steepest\PYZus{}increase\PYZus{}ot\PYZus{}the\PYZus{}probability.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}71}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_119_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}72}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}5\PYZhy{}9\PYZus{}reinforce\PYZus{}detailed\PYZus{}meaning\PYZus{}when\PYZus{}multiple\PYZus{}trajectories.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}72}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_120_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}73}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}5\PYZhy{}10\PYZus{}reinforce\PYZus{}pseudocode.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}73}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_121_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{reinforce-derivation-of-gradient}{%
\subsection{3-2-6 : REINFORCE Derivation of
Gradient}\label{reinforce-derivation-of-gradient}}

how to derive the equation that we use to approximate the gradient

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}74}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}6\PYZhy{}1\PYZus{}reinforce\PYZus{}derivation\PYZus{}of\PYZus{}equation\PYZus{}that\PYZus{}aproximate\PYZus{}the\PYZus{}gradient.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}74}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_123_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}75}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}6\PYZhy{}2\PYZus{}reinforce\PYZus{}derivation\PYZus{}of\PYZus{}equation\PYZus{}likelihood\PYZus{}ratio\PYZus{}policy\PYZus{}gradient.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}75}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_124_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{likelihood-ratio-policy-gradient}{%
\subsection{likelihood ratio policy
gradient}\label{likelihood-ratio-policy-gradient}}

\hypertarget{line-5-follows-from-the-chain-rule-and-the-fact-that-the-gradient-of-the-log-of-a-function-is-always-equal-to-the-gradient-of-the-function-divided-by-the-function.}{%
\subsubsection{* line (5) follows from the chain rule, and the fact that
the gradient of the log of a function is always equal to the gradient of
the function, divided by the
function.}\label{line-5-follows-from-the-chain-rule-and-the-fact-that-the-gradient-of-the-log-of-a-function-is-always-equal-to-the-gradient-of-the-function-divided-by-the-function.}}

\hypertarget{logpux3c4ux3b8-ux3b8pux3c4ux3b8-pux3c4ux3b8-is-referred-to-as-the-likelihood-ratio-trick-or-reinforce-trick}{%
\subsubsection{* logP(τ;θ)= ∇θP(τ;θ) / P(τ;θ) is referred to as the
likelihood ratio trick or REINFORCE
trick}\label{logpux3c4ux3b8-ux3b8pux3c4ux3b8-pux3c4ux3b8-is-referred-to-as-the-likelihood-ratio-trick-or-reinforce-trick}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}76}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}6\PYZhy{}3\PYZus{}reinforce\PYZus{}equation\PYZus{}likelihood\PYZus{}ratio\PYZus{}policy\PYZus{}gradient.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}76}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_126_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}77}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}6\PYZhy{}4\PYZus{}reinforce\PYZus{}derivation\PYZus{}of\PYZus{}equation\PYZus{}sample\PYZus{}based\PYZus{}estimate.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}77}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_127_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{likelihood-ratio-policy-gradient-with-a-sample-based-average}{%
\subsection{likelihood ratio policy gradient with a sample-based
average}\label{likelihood-ratio-policy-gradient-with-a-sample-based-average}}

\hypertarget{each-ux3c4i-is-a-sampled-trajectory.}{%
\subsubsection{* each τ(i) is a sampled
trajectory.}\label{each-ux3c4i-is-a-sampled-trajectory.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}78}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}6\PYZhy{}5\PYZus{}reinforce\PYZus{}derivation\PYZus{}of\PYZus{}equation\PYZus{}simplify.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}78}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_129_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{simplify-ux3b8logpux3c4iux3b8}{%
\subsection{Simplify
∇θlogP(τ(i);θ)}\label{simplify-ux3b8logpux3c4iux3b8}}

\hypertarget{line-1-shows-how-to-calculate-the-probability-of-an-arbitrary-trajectory-ux3c4i.}{%
\subsubsection{* line (1) shows how to calculate the probability of an
arbitrary trajectory
τ(i).}\label{line-1-shows-how-to-calculate-the-probability-of-an-arbitrary-trajectory-ux3c4i.}}

\hypertarget{line-4-because-ux3b8-has-no-dependence-on-ux3b8.-ux3b8-equation-is-0.}{%
\subsubsection{* line (4) because ``∇θ\ldots{}'' has no dependence on θ.
``∇θ\ldots{}'' equation is
0.}\label{line-4-because-ux3b8-has-no-dependence-on-ux3b8.-ux3b8-equation-is-0.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}79}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}2\PYZhy{}6\PYZhy{}6\PYZus{}reinforce\PYZus{}probility\PYZus{}density\PYZus{}function\PYZus{}correspoding\PYZus{}to\PYZus{}normal\PYZus{}distribution.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}79}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_131_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{whats-next}{%
\subsection{What's Next?}\label{whats-next}}

\hypertarget{so-far..-reinforce-is-used-to-solve-a-problem-in-discrete-action-space.}{%
\paragraph{- So far.. REINFORCE is used to solve a problem in discrete
action
space.}\label{so-far..-reinforce-is-used-to-solve-a-problem-in-discrete-action-space.}}

\hypertarget{reinforce-can-also-be-used-to-solve-environments-with-continuous-action-spaces}{%
\paragraph{- REINFORCE can also be used to solve environments with
continuous action
spaces!}\label{reinforce-can-also-be-used-to-solve-environments-with-continuous-action-spaces}}

\hypertarget{for-an-environment-with-a-continuous-action-space-the-corresponding-policy-network-could-have-an-output-layer-that-parametrizes-a-continuous-probability-distribution.}{%
\paragraph{- For an environment with a continuous action space, the
corresponding policy network could have an output layer that
parametrizes a continuous probability
distribution.}\label{for-an-environment-with-a-continuous-action-space-the-corresponding-policy-network-could-have-an-output-layer-that-parametrizes-a-continuous-probability-distribution.}}

\hypertarget{for-instance-assume-the-output-layer-returns-the-mean-ux3bc-and-variance-ux3c32-of-a-normal-distribution.}{%
\paragraph{- For instance, assume the output layer returns the mean
``μ'' and variance ``σ2'' of a normal
distribution.}\label{for-instance-assume-the-output-layer-returns-the-mean-ux3bc-and-variance-ux3c32-of-a-normal-distribution.}}

\hypertarget{then-in-order-to-select-an-action-the-agent-needs-only-to-pass-the-most-recent-state-state-as-input-to-the-network-and-then-use-the-output-mean-ux3bc-and-variance-ux3c32-to-sample-from-the-distribution-action-nux3bcux3c32.}{%
\paragraph{- Then in order to select an action, the agent needs only to
pass the most recent state ``State'' as input to the network, and then
use the output mean ``μ'' and variance ``σ2'' to sample from the
distribution ``Action'' \textasciitilde{}
∼N(μ,σ2).}\label{then-in-order-to-select-an-action-the-agent-needs-only-to-pass-the-most-recent-state-state-as-input-to-the-network-and-then-use-the-output-mean-ux3bc-and-variance-ux3c32-to-sample-from-the-distribution-action-nux3bcux3c32.}}

\hypertarget{we-will-see-the-small-modification-to-the-reinforce-algorithm}{%
\subsection{We will see the ``Small modification to the REINFORCE
algorithm''}\label{we-will-see-the-small-modification-to-the-reinforce-algorithm}}

    \hypertarget{lesson-3-3-proximal-policy-optimization}{%
\section{Lesson 3-3: Proximal Policy
Optimization}\label{lesson-3-3-proximal-policy-optimization}}

One of these key improvements is called Proximal Policy Optimization
(PPO) -- also closely related to Trust Region Policy Optimization
(TRPO). It has allowed faster and more stable learning. From developing
agile robots, to creating expert level gaming AI, PPO has proven useful
in a wide domain of applications, and has become part of the standard
toolkits in complicated learning environments.

\begin{itemize}
\tightlist
\item
  REINFORCE problems and issues
\item
  Solutions for REINFORCE problems will lead us to PPO.
\end{itemize}

\hypertarget{the-idea-of-ppo-was-published-by-the-team-at-openai-and-you-can-read-their-paper-through-this-link-httpsarxiv.orgabs1707.06347.}{%
\subsubsection{The idea of PPO was published by the team at OpenAI, and
you can read their paper through this {[}link{]}
\{https://arxiv.org/abs/1707.06347\}.}\label{the-idea-of-ppo-was-published-by-the-team-at-openai-and-you-can-read-their-paper-through-this-link-httpsarxiv.orgabs1707.06347.}}

    \hypertarget{beyond-reinforce}{%
\subsection{3-3-1 : Beyond REINFORCE}\label{beyond-reinforce}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}80}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}1\PYZhy{}1\PYZus{}beyond\PYZus{}reinforce\PYZus{}review\PYZus{}reinforce.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}80}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_135_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{main-problems-of-reinforce}{%
\subsection{Main problems of
REINFORCE}\label{main-problems-of-reinforce}}

\hypertarget{the-update-process-is-very-inefficient-we-run-the-policy-once-update-once-and-then-throw-away-the-trajectory.}{%
\subsubsection{1. The update process is very inefficient! We run the
policy once, update once, and then throw away the
trajectory.}\label{the-update-process-is-very-inefficient-we-run-the-policy-once-update-once-and-then-throw-away-the-trajectory.}}

\hypertarget{the-gradient-estimate-g-is-very-noisy.-by-chance-the-collected-trajectory-may-not-be-representative-of-the-policy.}{%
\subsubsection{2. The gradient estimate g is very noisy. By chance the
collected trajectory may not be representative of the
policy.}\label{the-gradient-estimate-g-is-very-noisy.-by-chance-the-collected-trajectory-may-not-be-representative-of-the-policy.}}

\hypertarget{there-is-no-clear-credit-assignment.-a-trajectory-may-contain-many-goodbad-actions-and-whether-these-actions-are-reinforced-depends-only-on-the-final-total-output.}{%
\subsubsection{3. There is no clear credit assignment. A trajectory may
contain many good/bad actions and whether these actions are reinforced
depends only on the final total
output.}\label{there-is-no-clear-credit-assignment.-a-trajectory-may-contain-many-goodbad-actions-and-whether-these-actions-are-reinforced-depends-only-on-the-final-total-output.}}

    \hypertarget{noise-reduction}{%
\subsection{3-3-2 : Noise Reduction}\label{noise-reduction}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}81}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}2\PYZhy{}1\PYZus{}noise\PYZus{}reduction\PYZus{}sampled\PYZus{}trajectories\PYZus{}do\PYZus{}not\PYZus{}contain\PYZus{}that\PYZus{}much\PYZus{}information\PYZus{}about\PYZus{}our\PYZus{}policy\PYZus{}because\PYZus{}of\PYZus{}random\PYZus{}noise.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}81}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_138_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}82}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}2\PYZhy{}2\PYZus{}noise\PYZus{}reduction\PYZus{}easiet\PYZus{}option\PYZus{}to\PYZus{}reduce\PYZus{}noise\PYZus{}is\PYZus{}simply\PYZus{}sample\PYZus{}more\PYZus{}trajectories.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}82}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_139_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}83}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}2\PYZhy{}3\PYZus{}noise\PYZus{}reduction\PYZus{}another\PYZus{}option\PYZus{}is\PYZus{}reward\PYZus{}normalization.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}83}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_140_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{reward-normalization}{%
\subsection{Reward Normalization}\label{reward-normalization}}

\hypertarget{running-multiple-trajectories-we-can-collect-all-the-total-rewards-and-get-a-sense-of-how-they-are-distributed.}{%
\subsubsection{- Running multiple trajectories: we can collect all the
total rewards and get a sense of how they are
distributed.}\label{running-multiple-trajectories-we-can-collect-all-the-total-rewards-and-get-a-sense-of-how-they-are-distributed.}}

\hypertarget{in-many-cases-the-distribution-of-rewards-shifts-as-learning-happens.-reward-1-might-be-really-good-in-the-beginning-but-really-bad-after-1000-training-episode.}{%
\subsubsection{- In many cases, the distribution of rewards shifts as
learning happens. Reward = 1 might be really good in the beginning, but
really bad after 1000 training
episode.}\label{in-many-cases-the-distribution-of-rewards-shifts-as-learning-happens.-reward-1-might-be-really-good-in-the-beginning-but-really-bad-after-1000-training-episode.}}

\hypertarget{this-batch-normalization-technique-is-also-used-in-many-other-problems-in-ai-e.g.image-classification-where-normalizing-the-input-can-improve-learning.}{%
\subsubsection{- This batch-normalization technique is also used in many
other problems in AI (e.g.~image classification), where normalizing the
input can improve
learning.}\label{this-batch-normalization-technique-is-also-used-in-many-other-problems-in-ai-e.g.image-classification-where-normalizing-the-input-can-improve-learning.}}

\hypertarget{intuitively-normalizing-the-rewards-roughly-corresponds-to-picking-half-the-actions-to-encouragediscourage-while-also-making-sure-the-steps-for-gradient-ascents-are-not-too-largesmall.}{%
\subsubsection{- Intuitively, normalizing the rewards roughly
corresponds to picking half the actions to encourage/discourage, while
also making sure the steps for gradient ascents are not too
large/small.}\label{intuitively-normalizing-the-rewards-roughly-corresponds-to-picking-half-the-actions-to-encouragediscourage-while-also-making-sure-the-steps-for-gradient-ascents-are-not-too-largesmall.}}

    \hypertarget{credit-assignment}{%
\subsection{3-3-3 : Credit Assignment}\label{credit-assignment}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}84}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}3\PYZhy{}1\PYZus{}credit\PYZus{}assignment\PYZus{}take\PYZus{}closer\PYZus{}look\PYZus{}at\PYZus{}the\PYZus{}total\PYZus{}reward\PYZus{}r.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}84}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_143_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}85}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}3\PYZhy{}2\PYZus{}credit\PYZus{}assignment\PYZus{}past\PYZus{}rewards\PYZus{}do\PYZus{}not\PYZus{}affect\PYZus{}to\PYZus{}current\PYZus{}action\PYZus{}we\PYZus{}assign\PYZus{}credit\PYZus{}to\PYZus{}current\PYZus{}action.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}85}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_144_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}86}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}3\PYZhy{}3\PYZus{}credit\PYZus{}assignment\PYZus{}simply\PYZus{}have\PYZus{}the\PYZus{}future\PYZus{}reward\PYZus{}as\PYZus{}the\PYZus{}coefficient.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}86}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_145_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{because-we-have-a-markov-process-the-action-at-time-step-t-can-only-affect-the-future-reward-so-the-past-reward-shouldnt-be-contributing-to-the-policy-gradient.-so-to-properly-assign-credit-to-the-action-a-we-should-ignore-the-past-reward.-so-a-better-policy-gradient-would-simply-have-the-future-reward-as-the-coefficient-.}{%
\subsection{Because we have a Markov process, the action at time-step t
can only affect the future reward, so the past reward shouldn't be
contributing to the policy gradient. So to properly assign credit to the
action ``a'', we should ignore the past reward. So a better policy
gradient would simply have the future reward as the coefficient
.}\label{because-we-have-a-markov-process-the-action-at-time-step-t-can-only-affect-the-future-reward-so-the-past-reward-shouldnt-be-contributing-to-the-policy-gradient.-so-to-properly-assign-credit-to-the-action-a-we-should-ignore-the-past-reward.-so-a-better-policy-gradient-would-simply-have-the-future-reward-as-the-coefficient-.}}

    \hypertarget{gradient-modification}{%
\subsection{Gradient Modification}\label{gradient-modification}}

\hypertarget{question-iswhy-is-it-okay-to-just-change-our-gradient-wouldnt-that-change-our-original-goal-of-maximizing-the-expected-reward}{%
\subsubsection{Question is\ldots{}``why is it okay to just change our
gradient? Wouldn't that change our original goal of maximizing the
expected
reward?''}\label{question-iswhy-is-it-okay-to-just-change-our-gradient-wouldnt-that-change-our-original-goal-of-maximizing-the-expected-reward}}

\hypertarget{it-turns-out-that-mathematically-ignoring-past-rewards-might-change-the-gradient-for-each-specific-trajectory-but-it-doesnt-change-the-averaged-gradient.}{%
\subsubsection{It turns out that mathematically, ignoring past rewards
might change the gradient for each specific trajectory, but it doesn't
change the averaged
gradient.}\label{it-turns-out-that-mathematically-ignoring-past-rewards-might-change-the-gradient-for-each-specific-trajectory-but-it-doesnt-change-the-averaged-gradient.}}

\hypertarget{so-even-though-the-gradient-is-different-during-training-on-average-we-are-still-maximizing-the-average-reward.}{%
\subsubsection{So even though the gradient is different during training,
on average we are still maximizing the average
reward.}\label{so-even-though-the-gradient-is-different-during-training-on-average-we-are-still-maximizing-the-average-reward.}}

\hypertarget{in-fact-the-resultant-gradient-is-less-noisy-so-training-using-future-reward-should-speed-things-up}{%
\subsection{In fact, the resultant gradient is less noisy, so training
using future reward should speed things
up!}\label{in-fact-the-resultant-gradient-is-less-noisy-so-training-using-future-reward-should-speed-things-up}}

    \hypertarget{quiz}{%
\subsection{Quiz}\label{quiz}}

Suppose we are training an agent to play a computer game. There are only
two possible action:

\begin{itemize}
\tightlist
\item
  0 = Do nothing, 1 = Move
\end{itemize}

There are three time-steps in each game, and our policy is completely
determined by one parameter θ, such that the probability of ``moving''
is θ, and the probability of doing nothing is 1−θ.

Initially θ=0.5. Three games are played, the results are:

\begin{itemize}
\tightlist
\item
  Game 1: actions: (1,0,1) rewards: (1,0,1)
\item
  Game 2: actions: (1,0,0) rewards: (0,0,1)
\item
  Game 3: actions: (0,1,0) rewards: (1,0,1)
\end{itemize}

    \hypertarget{question-1}{%
\subsubsection{Question 1}\label{question-1}}

What are the future rewards for the first game?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  (1,0,1)
\item
  (1,0,2)
\item
  (2,0,1)
\item
  (2,1,1)
\item
  (1,1,2)
\end{enumerate}

    \hypertarget{questin-2}{%
\subsubsection{Questin 2}\label{questin-2}}

What is the policy gradient computed from the second game, using future
rewards?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  -2
\item
  -1
\item
  0
\item
  1
\item
  2
\end{enumerate}

    \hypertarget{question-3}{%
\subsubsection{Question 3}\label{question-3}}

Which of these statements are true regarding the 3rd game?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We can add a baseline -1 point to the rewards, the computed tradient
  wouldn't change.
\item
  The contribution to the gradient from the second and third steps
  cancel each other.
\item
  The computed policy gradient from this game is 0.
\item
  The computed policy gradient from this game is negative.
\item
  Using the total reward vs future reward give the same policy gradient
  in this game.
\end{enumerate}

    \hypertarget{importance-sampling}{%
\subsection{3-3-4 : Importance Sampling}\label{importance-sampling}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}87}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}4\PYZhy{}1\PYZus{}importance\PYZus{}sampling\PYZus{}data\PYZus{}recycling.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}87}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_153_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}88}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}4\PYZhy{}2\PYZus{}importance\PYZus{}sampling\PYZus{}generate\PYZus{}trajectory\PYZus{}with\PYZus{}policy\PYZus{}pi\PYZus{}theta.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}88}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_154_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}89}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}4\PYZhy{}3\PYZus{}importance\PYZus{}sampling\PYZus{}compute\PYZus{}policy\PYZus{}gradient\PYZus{}and\PYZus{}update\PYZus{}theta\PYZus{}to\PYZus{}theta\PYZus{}prime.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}89}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_155_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}90}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}4\PYZhy{}4\PYZus{}importance\PYZus{}sampling\PYZus{}throw\PYZus{}away\PYZus{}just\PYZus{}generated\PYZus{}trajectory.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}90}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_156_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{if-we-want-to-update-our-policy-again-we-would-need-to-generate-new-trajectories-once-more-using-the-updated-policy.}{%
\subsection{If we want to update our policy again, we would need to
generate new trajectories once more, using the updated
policy.}\label{if-we-want-to-update-our-policy-again-we-would-need-to-generate-new-trajectories-once-more-using-the-updated-policy.}}

\hypertarget{you-might-ask-why-is-all-this-necessary-its-because-we-need-to-compute-the-gradient-for-the-current-policy-and-to-do-that-the-trajectories-need-to-be-representative-of-the-current-policy.}{%
\subsection{You might ask, why is all this necessary? It's because we
need to compute the gradient for the current policy, and to do that the
trajectories need to be representative of the current
policy.}\label{you-might-ask-why-is-all-this-necessary-its-because-we-need-to-compute-the-gradient-for-the-current-policy-and-to-do-that-the-trajectories-need-to-be-representative-of-the-current-policy.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}91}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}4\PYZhy{}5\PYZus{}importance\PYZus{}sampling\PYZus{}recycle\PYZus{}the\PYZus{}old\PYZus{}trajectories.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}91}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_158_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{what-if-we-could-somehow-recycle-the-old-trajectories-by-modifying-them-so-that-they-are-representative-of-the-new-policy-so-that-instead-of-just-throwing-them-away-we-recycle-them}{%
\subsection{What if we could somehow recycle the old trajectories, by
modifying them so that they are representative of the new policy? So
that instead of just throwing them away, we recycle
them!}\label{what-if-we-could-somehow-recycle-the-old-trajectories-by-modifying-them-so-that-they-are-representative-of-the-new-policy-so-that-instead-of-just-throwing-them-away-we-recycle-them}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}92}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}4\PYZhy{}6\PYZus{}importance\PYZus{}sampling\PYZus{}reuse\PYZus{}the\PYZus{}recycled\PYZus{}trajectories\PYZus{}to\PYZus{}compute\PYZus{}gradients\PYZus{}and\PYZus{}update\PYZus{}policy.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}92}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_160_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}93}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}4\PYZhy{}7\PYZus{}importance\PYZus{}sampling\PYZus{}generated\PYZus{}using\PYZus{}the\PYZus{}policy\PYZus{}pi\PYZus{}theta\PYZus{}is\PYZus{}same\PYZus{}trajectory\PYZus{}by\PYZus{}new\PYZus{}policy\PYZus{}different\PYZus{}probability.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}93}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_161_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}94}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}4\PYZhy{}8\PYZus{}importance\PYZus{}sampling\PYZus{}we\PYZus{}want\PYZus{}compute\PYZus{}the\PYZus{}average\PYZus{}of\PYZus{}some\PYZus{}quantity\PYZus{}say\PYZus{}f\PYZus{}of\PYZus{}Tau.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}94}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_162_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{imagine-we-want-to-compute-the-average-of-some-quantity-say-fux3c4.-we-could-simply-generate-trajectories-from-the-new-policy-compute-fux3c4-and-average-them.}{%
\subsection{Imagine we want to compute the average of some quantity, say
f(τ). We could simply generate trajectories from the new policy, compute
f(τ) and average
them.}\label{imagine-we-want-to-compute-the-average-of-some-quantity-say-fux3c4.-we-could-simply-generate-trajectories-from-the-new-policy-compute-fux3c4-and-average-them.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}95}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}4\PYZhy{}9\PYZus{}importance\PYZus{}sampling\PYZus{}weighted\PYZus{}by\PYZus{}a\PYZus{}probability\PYZus{}of\PYZus{}sampling\PYZus{}each\PYZus{}trajectory.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}95}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_164_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{mathematically-this-is-equivalent-to-adding-up-all-the-fux3c4-weighted-by-a-probability-of-sampling-each-trajectory-under-the-new-policy.}{%
\subsection{Mathematically, this is equivalent to adding up all the
f(τ), weighted by a probability of sampling each trajectory under the
new
policy.}\label{mathematically-this-is-equivalent-to-adding-up-all-the-fux3c4-weighted-by-a-probability-of-sampling-each-trajectory-under-the-new-policy.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}96}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}4\PYZhy{}10\PYZus{}multiplying\PYZus{}and\PYZus{}dividing\PYZus{}by\PYZus{}the\PYZus{}same\PYZus{}number\PYZus{}P(τ;θ).jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}96}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_166_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}97}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}4\PYZhy{}11\PYZus{}importance\PYZus{}sampling\PYZus{}rearrange\PYZus{}the\PYZus{}terms.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}97}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_167_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}98}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}4\PYZhy{}12\PYZus{}importance\PYZus{}sampling\PYZus{}under\PYZus{}the\PYZus{}old\PYZus{}policy\PYZus{}and\PYZus{}extra\PYZus{}re\PYZhy{}weighting\PYZus{}factor\PYZus{}in\PYZus{}addition\PYZus{}to\PYZus{}just\PYZus{}averaging.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}98}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_168_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{intuitively-this-tells-us-we-can-use-old-trajectories-for-computing-averages-for-new-policy-as-long-as-we-add-this-extra-re-weighting-factor-that-takes-into-account-how-under-or-overrepresented-each-trajectory-is-under-the-new-policy-compared-to-the-old-one.}{%
\subsection{Intuitively, this tells us we can use old trajectories for
computing averages for new policy, as long as we add this extra
re-weighting factor, that takes into account how under or
over--represented each trajectory is under the new policy compared to
the old
one.}\label{intuitively-this-tells-us-we-can-use-old-trajectories-for-computing-averages-for-new-policy-as-long-as-we-add-this-extra-re-weighting-factor-that-takes-into-account-how-under-or-overrepresented-each-trajectory-is-under-the-new-policy-compared-to-the-old-one.}}

\hypertarget{the-same-tricks-are-used-frequently-across-statistics-where-the-re-weighting-factor-is-included-to-un-bias-surveys-and-voting-predictions.}{%
\subsection{The same tricks are used frequently across statistics, where
the re-weighting factor is included to un-bias surveys and voting
predictions.}\label{the-same-tricks-are-used-frequently-across-statistics-where-the-re-weighting-factor-is-included-to-un-bias-surveys-and-voting-predictions.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}99}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
         \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}4\PYZhy{}13\PYZus{}importance\PYZus{}sampling\PYZus{}re\PYZhy{}weighting\PYZus{}factor.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}99}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_170_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{because-each-trajectory-contains-many-steps-the-probability-contains-a-chain-of-products-of-each-policy-at-different-time-step.}{%
\subsection{Because each trajectory contains many steps, the probability
contains a chain of products of each policy at different
time-step.}\label{because-each-trajectory-contains-many-steps-the-probability-contains-a-chain-of-products-of-each-policy-at-different-time-step.}}

\hypertarget{this-formula-is-a-bit-complicated.-but-there-is-a-bigger-problem.-when-some-of-policy-gets-close-to-zero-the-re-weighting-factor-can-become-close-to-zero-or-worse-close-to-1-over-0-which-diverges-to-infinity.}{%
\subsection{This formula is a bit complicated. But there is a bigger
problem. When some of policy gets close to zero, the re-weighting factor
can become close to zero, or worse, close to 1 over 0 which diverges to
infinity.}\label{this-formula-is-a-bit-complicated.-but-there-is-a-bigger-problem.-when-some-of-policy-gets-close-to-zero-the-re-weighting-factor-can-become-close-to-zero-or-worse-close-to-1-over-0-which-diverges-to-infinity.}}

\hypertarget{when-this-happens-the-re-weighting-trick-becomes-unreliable.-so-in-practice-we-want-to-make-sure-the-re-weighting-factor-is-not-too-far-from-1-when-we-utilize-importance-sampling}{%
\subsection{When this happens, the re-weighting trick becomes
unreliable. So, In practice, we want to make sure the re-weighting
factor is not too far from 1 when we utilize importance
sampling}\label{when-this-happens-the-re-weighting-trick-becomes-unreliable.-so-in-practice-we-want-to-make-sure-the-re-weighting-factor-is-not-too-far-from-1-when-we-utilize-importance-sampling}}

    \hypertarget{ppo-part-1-the-surrogate-function}{%
\subsection{3-3-5 : PPO Part 1: The Surrogate
Function}\label{ppo-part-1-the-surrogate-function}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}100}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}5\PYZhy{}1\PYZus{}ppo\PYZus{}surrogate\PYZus{}function.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}100}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_173_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}101}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}5\PYZhy{}2\PYZus{}ppo\PYZus{}surrogate\PYZus{}function\PYZus{}re\PYZhy{}weighining\PYZus{}pg\PYZus{}which\PYZus{}is\PYZus{}applied\PYZus{}reward\PYZus{}normalization\PYZus{}and\PYZus{}credit\PYZus{}assignment.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}101}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_174_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}102}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}5\PYZhy{}3\PYZus{}ppo\PYZus{}surrogate\PYZus{}function\PYZus{}times\PYZus{}a\PYZus{}re\PYZhy{}weighting\PYZus{}factor.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}102}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_175_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}103}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}5\PYZhy{}4\PYZus{}ppo\PYZus{}surrogate\PYZus{}function\PYZus{}rearrange\PYZus{}equation.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}103}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_176_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}104}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}5\PYZhy{}5\PYZus{}ppo\PYZus{}surrogate\PYZus{}function\PYZus{}re\PYZhy{}weighting\PYZus{}factor\PYZus{}is\PYZus{}ust\PYZus{}the\PYZus{}product\PYZus{}of\PYZus{}all\PYZus{}the\PYZus{}policy\PYZus{}across\PYZus{}each\PYZus{}step.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}104}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_177_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}105}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}5\PYZhy{}6\PYZus{}ppo\PYZus{}surrogate\PYZus{}function\PYZus{}rearrange\PYZus{}equation\PYZus{}again.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}105}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_178_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}106}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}5\PYZhy{}7\PYZus{}ppo\PYZus{}surrogate\PYZus{}function\PYZus{}cancel\PYZus{}some\PYZus{}terms.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}106}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_179_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{this-is-where-proximal-policy-comes-in.-if-the-old-and-current-policy-is-close-enough-to-each-other-all-the-factors-inside-the-would-be-pretty-close-to-1-and-then-we-can-ignore-them.}{%
\subsection{This is where proximal policy comes in. If the old and
current policy is close enough to each other, all the factors inside the
``\ldots{}'' would be pretty close to 1, and then we can ignore
them.}\label{this-is-where-proximal-policy-comes-in.-if-the-old-and-current-policy-is-close-enough-to-each-other-all-the-factors-inside-the-would-be-pretty-close-to-1-and-then-we-can-ignore-them.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}107}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}5\PYZhy{}8\PYZus{}ppo\PYZus{}surrogate\PYZus{}function\PYZus{}equation\PYZus{}simplified.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}107}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_181_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{it-looks-very-similar-to-the-old-policy-gradient.-in-fact-if-the-current-policy-and-the-old-policy-is-the-same-we-would-have-exactly-the-vanilla-policy-gradient.-but-remember-this-expression-is-different-because-we-are-comparing-two-different-policies}{%
\subsection{It looks very similar to the old policy gradient. In fact,
if the current policy and the old policy is the same, we would have
exactly the vanilla policy gradient. But remember, this expression is
different because we are comparing two different
policies}\label{it-looks-very-similar-to-the-old-policy-gradient.-in-fact-if-the-current-policy-and-the-old-policy-is-the-same-we-would-have-exactly-the-vanilla-policy-gradient.-but-remember-this-expression-is-different-because-we-are-comparing-two-different-policies}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}108}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}5\PYZhy{}9\PYZus{}ppo\PYZus{}surrogate\PYZus{}function\PYZus{}equation\PYZus{}rearranged.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}108}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_183_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}109}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}5\PYZhy{}10\PYZus{}ppo\PYZus{}surrogate\PYZus{}function\PYZus{}now\PYZus{}we\PYZus{}have\PYZus{}the\PYZus{}approximate\PYZus{}form\PYZus{}of\PYZus{}the\PYZus{}gradient.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}109}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_184_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}110}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}5\PYZhy{}11\PYZus{}ppo\PYZus{}surrogate\PYZus{}function\PYZus{}new\PYZus{}gradient.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}110}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_185_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{so-using-this-new-gradient-we-can-perform-gradient-ascent-to-update-our-policy-which-can-be-thought-as-directly-maximize-the-surrogate-function.}{%
\subsection{* So using this new gradient, we can perform gradient ascent
to update our policy -- which can be thought as directly maximize the
surrogate
function.}\label{so-using-this-new-gradient-we-can-perform-gradient-ascent-to-update-our-policy-which-can-be-thought-as-directly-maximize-the-surrogate-function.}}

\hypertarget{but-there-is-still-one-important-issue-we-havent-addressed-yet.-if-we-keep-reusing-old-trajectories-and-updating-our-policy-at-some-point-the-new-policy-might-become-different-enough-from-the-old-one-so-that-all-the-approximations-we-made-could-become-invalid.}{%
\subsection{* But there is still one important issue we haven't
addressed yet. If we keep reusing old trajectories and updating our
policy, at some point the new policy might become different enough from
the old one, so that all the approximations we made could become
invalid.}\label{but-there-is-still-one-important-issue-we-havent-addressed-yet.-if-we-keep-reusing-old-trajectories-and-updating-our-policy-at-some-point-the-new-policy-might-become-different-enough-from-the-old-one-so-that-all-the-approximations-we-made-could-become-invalid.}}

    \hypertarget{ppo-part-2-clipping-policy-updates}{%
\subsection{3-3-6 : PPO Part 2: Clipping Policy
Updates}\label{ppo-part-2-clipping-policy-updates}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}111}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}6\PYZhy{}1\PYZus{}ppo\PYZus{}clipping\PYZus{}policy\PYZus{}updates.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}111}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_188_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}112}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}6\PYZhy{}2\PYZus{}ppo\PYZus{}the\PYZus{}policy\PYZus{}or\PYZus{}reward\PYZus{}off.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}112}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_189_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{what-is-the-problem-with-updating-our-policy-and-ignoring-the-fact-that-the-approximations-are-not-valid-anymore-one-problem-is-it-could-lead-to-a-really-bad-policy-that-is-very-hard-to-recover-from.-lets-see-how}{%
\subsection{* What is the problem with updating our policy and ignoring
the fact that the approximations are not valid anymore? One problem is
it could lead to a really bad policy that is very hard to recover from.
Let's see
how\textasciitilde{}}\label{what-is-the-problem-with-updating-our-policy-and-ignoring-the-fact-that-the-approximations-are-not-valid-anymore-one-problem-is-it-could-lead-to-a-really-bad-policy-that-is-very-hard-to-recover-from.-lets-see-how}}

\hypertarget{say-we-have-some-policy-parameterized-by-ux3c0ux3b8shown-on-the-left-plot-in-black-and-with-an-average-reward-function-shown-on-the-right-plot-in-black.}{%
\subsection{* Say we have some policy parameterized by πθ′(shown on the
left plot in black), and with an average reward function (shown on the
right plot in
black).}\label{say-we-have-some-policy-parameterized-by-ux3c0ux3b8shown-on-the-left-plot-in-black-and-with-an-average-reward-function-shown-on-the-right-plot-in-black.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}113}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}6\PYZhy{}3\PYZus{}ppo\PYZus{}Lsur\PYZus{}approximates\PYZus{}reward\PYZus{}well\PYZus{}around\PYZus{}the\PYZus{}current\PYZus{}policy\PYZus{}but\PYZus{}diverges\PYZus{}from\PYZus{}actual\PYZus{}reward.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}113}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_191_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{the-current-policy-is-labelled-by-the-red-text-and-the-goal-is-to-update-the-current-policy-to-the-optimal-one-green-star.}{%
\subsection{* The current policy is labelled by the red text, and the
goal is to update the current policy to the optimal one (green
star).}\label{the-current-policy-is-labelled-by-the-red-text-and-the-goal-is-to-update-the-current-policy-to-the-optimal-one-green-star.}}

\hypertarget{to-update-the-policy-we-can-compute-a-surrogate-function-lsurdotted-red-curve-on-right-plot.-so-lsur-approximates-the-reward-pretty-well-around-the-current-policy.-but-far-away-from-the-current-policy-it-diverges-from-the-actual-reward.}{%
\subsection{* To update the policy we can compute a surrogate function
Lsur(dotted-red curve on right plot). So Lsur approximates the reward
pretty well around the current policy. But far away from the current
policy, it diverges from the actual
reward.}\label{to-update-the-policy-we-can-compute-a-surrogate-function-lsurdotted-red-curve-on-right-plot.-so-lsur-approximates-the-reward-pretty-well-around-the-current-policy.-but-far-away-from-the-current-policy-it-diverges-from-the-actual-reward.}}

\hypertarget{if-we-continually-update-the-policy-by-performing-gradient-ascent-we-might-get-something-like-the-red-dots.-the-big-problem-is-that-at-some-point-we-hit-a-cliff-where-the-policy-changes-by-a-large-amount.-from-the-perspective-of-the-surrogate-function-the-average-reward-is-really-great.-but-the-actually-average-reward-is-really-bad}{%
\subsection{* If we continually update the policy by performing gradient
ascent, we might get something like the red-dots. The big problem is
that at some point we hit a cliff, where the policy changes by a large
amount. From the perspective of the surrogate function, the average
reward is really great. But the actually average reward is really
bad!}\label{if-we-continually-update-the-policy-by-performing-gradient-ascent-we-might-get-something-like-the-red-dots.-the-big-problem-is-that-at-some-point-we-hit-a-cliff-where-the-policy-changes-by-a-large-amount.-from-the-perspective-of-the-surrogate-function-the-average-reward-is-really-great.-but-the-actually-average-reward-is-really-bad}}

\hypertarget{whats-worse-the-policy-is-now-stuck-in-a-deep-and-flat-bottom-so-that-future-updates-wont-be-able-to-bring-the-policy-back-up-we-are-now-stuck-with-a-really-bad-policy.}{%
\subsection{* What's worse, the policy is now stuck in a deep and flat
bottom, so that future updates won't be able to bring the policy back
up! we are now stuck with a really bad
policy.}\label{whats-worse-the-policy-is-now-stuck-in-a-deep-and-flat-bottom-so-that-future-updates-wont-be-able-to-bring-the-policy-back-up-we-are-now-stuck-with-a-really-bad-policy.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}114}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}6\PYZhy{}4\PYZus{}ppo\PYZus{}clipped\PYZus{}surrogate\PYZus{}function.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}114}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_193_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}115}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}6\PYZhy{}5\PYZus{}ppo\PYZus{}clipped\PYZus{}surrogate\PYZus{}function\PYZus{}if\PYZus{}reward\PYZus{}function\PYZus{}is\PYZus{}zero\PYZus{}the\PYZus{}gradient\PYZus{}zero\PYZus{}policy\PYZus{}update\PYZus{}will\PYZus{}stop.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}115}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_194_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{so-starting-with-the-current-policy-blue-dot-we-apply-gradient-ascent.-the-updates-remain-the-same-until-we-hit-the-flat-plateau.-now-because-the-reward-function-is-flat-the-gradient-is-zero-and-the-policy-update-will-stop}{%
\subsection{* So starting with the current policy (blue dot), we apply
gradient ascent. The updates remain the same, until we hit the flat
plateau. Now because the reward function is flat, the gradient is zero,
and the policy update will
stop!}\label{so-starting-with-the-current-policy-blue-dot-we-apply-gradient-ascent.-the-updates-remain-the-same-until-we-hit-the-flat-plateau.-now-because-the-reward-function-is-flat-the-gradient-is-zero-and-the-policy-update-will-stop}}

\hypertarget{now-keep-in-mind-that-we-are-only-showing-a-2d-figure-with-one-ux3b8-direction.-in-most-cases-there-are-thousands-of-parameters-in-a-policy-and-there-may-be-hundredsthousands-of-high-dimensional-cliffs-in-many-different-directions.-we-need-to-apply-this-clipping-mathematically-so-that-it-will-automatically-take-care-of-all-the-cliffs.}{%
\subsection{* Now, keep in mind that we are only showing a 2D figure
with one θ′ direction. In most cases, there are thousands of parameters
in a policy, and there may be hundreds/thousands of high-dimensional
cliffs in many different directions. We need to apply this clipping
mathematically so that it will automatically take care of all the
cliffs.}\label{now-keep-in-mind-that-we-are-only-showing-a-2d-figure-with-one-ux3b8-direction.-in-most-cases-there-are-thousands-of-parameters-in-a-policy-and-there-may-be-hundredsthousands-of-high-dimensional-cliffs-in-many-different-directions.-we-need-to-apply-this-clipping-mathematically-so-that-it-will-automatically-take-care-of-all-the-cliffs.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}116}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}6\PYZhy{}6\PYZus{}ppo\PYZus{}clipped\PYZus{}surrogate\PYZus{}function\PYZus{}original surrogate function.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}116}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_196_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{the-black-dot-shows-the-location-where-the-current-policy-is-the-same-as-the-old-policy-ux3b8ux3b8}{%
\subsection{The black dot shows the location where the current policy is
the same as the old policy
(θ′=θ)}\label{the-black-dot-shows-the-location-where-the-current-policy-is-the-same-as-the-old-policy-ux3b8ux3b8}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}117}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}6\PYZhy{}7\PYZus{}ppo\PYZus{}clipped\PYZus{}surrogate\PYZus{}function\PYZus{}apply\PYZus{}the\PYZus{}clip\PYZus{}function\PYZus{}to\PYZus{}force\PYZus{}the\PYZus{}ratio\PYZus{}to\PYZus{}be\PYZus{}within\PYZus{}the\PYZus{}interval.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}117}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_198_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{we-want-to-make-sure-the-two-policy-is-similar-or-that-the-ratio-is-close-to-1.-so-we-choose-a-small-ux3f5-typically-0.1-or-0.2-and-apply-the-clip-function-to-force-the-ratio-to-be-within-the-interval-1ux3f51ux3f5-shown-in-purple.}{%
\subsection{We want to make sure the two policy is similar, or that the
ratio is close to 1. So we choose a small ϵ (typically 0.1 or 0.2), and
apply the clip function to force the ratio to be within the interval
{[}1−ϵ,1+ϵ{]} (shown in
purple).}\label{we-want-to-make-sure-the-two-policy-is-similar-or-that-the-ratio-is-close-to-1.-so-we-choose-a-small-ux3f5-typically-0.1-or-0.2-and-apply-the-clip-function-to-force-the-ratio-to-be-within-the-interval-1ux3f51ux3f5-shown-in-purple.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}118}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}6\PYZhy{}8\PYZus{}ppo\PYZus{}clipped\PYZus{}surrogate\PYZus{}function\PYZus{}we\PYZus{}only\PYZus{}want\PYZus{}to\PYZus{}clip\PYZus{}the\PYZus{}top\PYZus{}part\PYZus{}and\PYZus{}not\PYZus{}the\PYZus{}bottom\PYZus{}part.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}118}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_200_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{now-the-ratio-is-clipped-in-two-places.-but-we-only-want-to-clip-the-top-part-and-not-the-bottom-part.-to-do-that-we-compare-this-clipped-ratio-to-the-original-one-and-take-the-minimum-show-in-blue.}{%
\subsection{Now the ratio is clipped in two places. But we only want to
clip the top part and not the bottom part. To do that, we compare this
clipped ratio to the original one and take the minimum (show in
blue).}\label{now-the-ratio-is-clipped-in-two-places.-but-we-only-want-to-clip-the-top-part-and-not-the-bottom-part.-to-do-that-we-compare-this-clipped-ratio-to-the-original-one-and-take-the-minimum-show-in-blue.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}119}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}6\PYZhy{}9\PYZus{}ppo\PYZus{}clipped\PYZus{}surrogate\PYZus{}function\PYZus{}gives\PYZus{}us\PYZus{}more\PYZus{}conservative\PYZus{}reward.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}119}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_202_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{this-then-ensures-the-clipped-surrogate-function-is-always-less-than-the-original-surrogate-function-lsurclip-lsur-so-the-clipped-surrogate-function-gives-a-more-conservative-reward.}{%
\subsection{This then ensures the clipped surrogate function is always
less than the original surrogate function Lsurclip ≤ Lsur, so the
clipped surrogate function gives a more conservative
``reward''.}\label{this-then-ensures-the-clipped-surrogate-function-is-always-less-than-the-original-surrogate-function-lsurclip-lsur-so-the-clipped-surrogate-function-gives-a-more-conservative-reward.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}120}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}6\PYZhy{}10\PYZus{}ppo\PYZus{}summary.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}120}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_204_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \hypertarget{the-details-of-ppo-was-originally-published-by-the-team-at-openai-and-you-can-read-their-paper-through-this-linkhttpsarxiv.orgabs1707.06347.}{%
\section{The details of PPO was originally published by the team at
OpenAI, and you can read their paper through this
{[}link{]}\{https://arxiv.org/abs/1707.06347\}.}\label{the-details-of-ppo-was-originally-published-by-the-team-at-openai-and-you-can-read-their-paper-through-this-linkhttpsarxiv.orgabs1707.06347.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}121}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
          \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./images/3\PYZhy{}3\PYZhy{}6\PYZhy{}11\PYZus{}ppo\PYZus{}paper.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}121}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_206_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
